{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdxR6WIQY-TI",
        "outputId": "44013d60-652e-42bc-f251-5b6b34a0cc98"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘sp’, ‘terra’\n",
            "\n",
            "\n",
            "\n",
            "raster installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘here’, ‘png’, ‘config’, ‘reticulate’, ‘tfruns’, ‘tfautograph’\n",
            "\n",
            "\n",
            "\n",
            "tensorflow installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependency ‘zeallot’\n",
            "\n",
            "\n",
            "\n",
            "keras installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "\n",
            "tfdatasets installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘listenv’, ‘parallelly’, ‘future’, ‘globals’, ‘warp’, ‘furrr’, ‘slider’\n",
            "\n",
            "\n",
            "\n",
            "rsample installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "\n",
            "rgdal installed\n",
            "\n",
            "Downloading GitHub repo rstudio/keras@HEAD\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "backports (1.2.1 -> 1.3.0) [CRAN]\n",
            "generics  (0.1.0 -> 0.1.1) [CRAN]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing 2 packages: backports, generics\n",
            "\n",
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking for file ‘/tmp/RtmpgoopDv/remotes4f5eab5d00/rstudio-keras-3c40170/DESCRIPTION’\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mpreparing ‘keras’:\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[36m (643ms)\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n",
            "   Removed empty directory ‘keras/man-roxygen’\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mbuilding ‘keras_2.6.1.9000.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n",
            "   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#@title **Install packages (~4 min)**\n",
        "install.packages(\"pacman\")\n",
        "library(pacman)\n",
        "\n",
        "# install packages for deep learning - ~4 min to load\n",
        "p_load(raster, sp)\n",
        "p_load(devtools, tensorflow, reticulate, keras, tfdatasets, tidyverse, rsample, rgdal)\n",
        "#new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]\n",
        "#if(length(new.packages)) install.packages(new.packages)\n",
        "#lapply(list.of.packages, require, character.only = TRUE)\n",
        "devtools::install_github(\"rstudio/keras\")\n",
        "p_load(keras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pu-GWF21iChM"
      },
      "outputs": [],
      "source": [
        "# # for rgee\n",
        "system('sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable')\n",
        "system('sudo apt-get update')\n",
        "system('sudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev')\n",
        "system('sudo apt-get install libprotobuf-dev protobuf-compiler libv8-dev libjq-dev')\n",
        "p_load(sf, mapview, cptcity, geojsonio)\n",
        "install.packages('sf')\n",
        "install.packages('mapview')\n",
        "install.packages('cptcity')\n",
        "install.packages('geojsonio')\n",
        "remotes::install_github(\"r-spatial/rgee@rgeev.1.0.3\")\n",
        "p_load(leaflet, rgdal, raster, sp, rgeos, gdalUtils, parallel, doParallel, foreach, fasterize, spatstat, maptools)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnLsxoCUxa61",
        "outputId": "a9aac324-2dea-43fc-ad21-1193f5d3544c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘promises’, ‘later’, ‘plyr’, ‘httpuv’, ‘units’, ‘leafem’, ‘leafpop’, ‘satellite’, ‘servr’, ‘sf’, ‘webshot’\n",
            "\n",
            "\n",
            "Warning message in install.packages(\"mapview\"):\n",
            "“installation of package ‘satellite’ had non-zero exit status”\n",
            "Warning message in install.packages(\"mapview\"):\n",
            "“installation of package ‘leafem’ had non-zero exit status”\n",
            "Warning message in install.packages(\"mapview\"):\n",
            "“installation of package ‘mapview’ had non-zero exit status”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘triebeard’, ‘urltools’, ‘httpcode’, ‘protolite’, ‘rgeos’, ‘crul’, ‘maptools’, ‘V8’, ‘geojson’, ‘jqr’\n",
            "\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Downloading GitHub repo r-spatial/rgee@rgeev.1.0.3\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking for file ‘/tmp/RtmpHhNNBs/remotes4e75c3977b/r-spatial-rgee-1a0c0ce/DESCRIPTION’\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mpreparing ‘rgee’:\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n",
            "   Omitted ‘LazyData’ from DESCRIPTION\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mbuilding ‘rgee_1.0.3.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n",
            "   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "\n",
            "rgeos installed\n",
            "\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘leafem’, ‘satellite’\n",
            "\n",
            "\n",
            "Warning message in utils::install.packages(package, ...):\n",
            "“installation of package ‘leafem’ had non-zero exit status”\n",
            "Warning message in utils::install.packages(package, ...):\n",
            "“installation of package ‘satellite’ had non-zero exit status”\n",
            "Warning message in utils::install.packages(package, ...):\n",
            "“installation of package ‘mapview’ had non-zero exit status”\n",
            "Warning message in p_install(package, character.only = TRUE, ...):\n",
            "“”\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘mapview’”\n",
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "\n",
            "geojsonio installed\n",
            "\n",
            "Warning message in p_load(rgeos, sp, mapview, geojsonio, cptcity):\n",
            "“Failed to install/load:\n",
            "rgeos, mapview, geojsonio”\n"
          ]
        }
      ],
      "source": [
        "# # for rgee\n",
        "system('sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable')\n",
        "system('sudo apt-get update')\n",
        "system('sudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev')\n",
        "system('sudo apt-get install libprotobuf-dev protobuf-compiler libv8-dev libjq-dev')\n",
        "install.packages('mapview')\n",
        "install.packages('cptcity')\n",
        "install.packages('rgeos')\n",
        "install.packages('sp')\n",
        "remotes::install_github(\"r-spatial/rgee@rgeev.1.0.3\")\n",
        "\n",
        "# libraries\n",
        "p_load(rgeos, sp, cptcity)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eE5RXmcy3E0E",
        "outputId": "12e1cbe0-0ef3-49d7-ba8a-18160b400e65"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Downloading GitHub repo gearslaboratory/gdalUtils@HEAD\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "R.methodsS3 (NA -> 1.8.1 ) [CRAN]\n",
            "R.oo        (NA -> 1.24.0) [CRAN]\n",
            "iterators   (NA -> 1.0.13) [CRAN]\n",
            "R.utils     (NA -> 2.11.0) [CRAN]\n",
            "foreach     (NA -> 1.5.1 ) [CRAN]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing 5 packages: R.methodsS3, R.oo, iterators, R.utils, foreach\n",
            "\n",
            "Installing packages into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking for file ‘/tmp/RtmpHhNNBs/remotes4e16b95d79/gearslaboratory-gdalUtils-8872aaf/DESCRIPTION’\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mpreparing ‘gdalUtils’:\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mbuilding ‘gdalUtils_2.0.3.2.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n",
            "   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Warning message in i.p(...):\n",
            "“installation of package ‘/tmp/RtmpHhNNBs/file4e7bf536b2/gdalUtils_2.0.3.2.tar.gz’ had non-zero exit status”\n"
          ]
        }
      ],
      "source": [
        "install.packages(\"devtools\")\n",
        "\n",
        "devtools:::install_github(\"gearslaboratory/gdalUtils\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMDVMV4v7zlf",
        "outputId": "77f602dd-cebc-42ab-8ec5-8b0a90318cd8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading GitHub repo r-spatial/sf@HEAD\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking for file ‘/tmp/RtmpHhNNBs/remotes4e54f0dd07/r-spatial-sf-0e8bad6/DESCRIPTION’\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mpreparing ‘sf’:\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[32m✔\u001b[39m  \u001b[90mchecking DESCRIPTION meta-information\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mcleaning src\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mrunning ‘cleanup’\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for LF line-endings in source and make files and shell scripts\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mchecking for empty or unneeded directories\u001b[39m\u001b[36m\u001b[39m\n",
            "\u001b[90m─\u001b[39m\u001b[90m  \u001b[39m\u001b[90mbuilding ‘sf_1.0-4.tar.gz’\u001b[39m\u001b[36m\u001b[39m\n",
            "   \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "library(devtools)\n",
        "install_github(\"r-spatial/sf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68D93PyXC1SX",
        "outputId": "11a01c08-6dca-45df-b688-4f75e55afab9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "Warning message in utils::install.packages(package, ...):\n",
            "“installation of package ‘gdalUtils’ had non-zero exit status”\n",
            "Warning message in p_install(package, character.only = TRUE, ...):\n",
            "“”\n",
            "Warning message in library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE, :\n",
            "“there is no package called ‘gdalUtils’”\n",
            "Warning message in p_load(gdalUtils):\n",
            "“Failed to install/load:\n",
            "gdalUtils”\n"
          ]
        }
      ],
      "source": [
        "p_load(gdalUtils)\n",
        "\n",
        "p_load(gdalUtils, sf, mapview, geojsonio)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgZQNUk68OKJ",
        "outputId": "cabbc9cd-d0c4-4292-cf68-8a70171b375f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "install.packages('sf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WiAy6pHFvJB",
        "outputId": "fedf76bc-6cec-407f-80e1-9b8a2c56c24a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Installing package into ‘/usr/local/lib/R/site-library’\n",
            "(as ‘lib’ is unspecified)\n",
            "\n",
            "also installing the dependencies ‘proxy’, ‘e1071’, ‘wk’, ‘classInt’, ‘s2’, ‘units’\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "system('sudo add-apt-repository ppa:ubuntugis/ubuntugis-unstable')\n",
        "system('sudo apt-get update')\n",
        "system('sudo apt-get install libudunits2-dev libgdal-dev libgeos-dev libproj-dev')\n",
        "install.packages('sf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 95
        },
        "id": "BXYs9xCOGt8s",
        "outputId": "5c036267-41cc-409b-9fdb-47dcda908445"
      },
      "outputs": [
        {
          "ename": "ERROR",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "Error in p_load(sf): could not find function \"p_load\"\nTraceback:\n"
          ]
        }
      ],
      "source": [
        "p_load(sf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoiIQBuskZM1"
      },
      "outputs": [],
      "source": [
        "#@title Download datasets\n",
        "# unzip\n",
        "unzip(zipfile='/content/input_training.zip', exdir=\"5_sampling1_singledate/input\")\n",
        "unzip(zipfile='/content/input_prediction.zip', exdir=\"6_sampling1_prediction_singledate/pred_input\")\n",
        "unzip(zipfile='/content/input_shapefiles.zip', exdir=\"4_shapes\")\n",
        "\n",
        "# download unet weight\n",
        "dir.create(\"/content/5_sampling1_singledate/weights_r_save/\")\n",
        "download.file('https://zenodo.org/record/5498488/files/unet_tf2_385_0.9311.h5?download=1', '/content/5_sampling1_singledate/weights_r_save/unet_tf2_385_0.9311.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fPKmgvwuB67"
      },
      "outputs": [],
      "source": [
        "#@title Set some variables\n",
        "\n",
        "# set up deep learning\n",
        "training_data_dir = \"5_sampling1_singledate\"\n",
        "prediction_data_dir = \"6_sampling1_prediction_singledate\"\n",
        "#\n",
        "img_dir = paste0(training_data_dir, \"/input/image\")\n",
        "class_dir = paste0(training_data_dir,\"/input/class\")\n",
        "result_fname = \"palms_map\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vls2-eY_tDmm"
      },
      "outputs": [],
      "source": [
        "#@title Functions\n",
        "# function to extract data from a raster (x) from a SpatialPolygons object (y) faster than extract()\n",
        "fast_extract = function(x,y) {\n",
        "  value = list()\n",
        "  for (i in 1:length(y)) {\n",
        "    tmp = crop(x, y[i,])\n",
        "    #value[[i]] = extract(tmp, y[i,])\n",
        "    value[[i]] = as.numeric(na.omit(mask(tmp, y[i,])[]))\n",
        "    rm(tmp)\n",
        "  }\n",
        "  return(value)\n",
        "}\n",
        "\n",
        "# function to extract data from a raster (x) from a SpatialPolygons object (y) faster than extract()\n",
        "fast_extract_parallel = function(x,y) {\n",
        "\n",
        "  # libraries needed\n",
        "  require(parallel) # install.packages(\"parallel\")\n",
        "  require(doParallel) # install.packages(\"doParallel\")\n",
        "  require(foreach) # install.packages(\"foreach\")\n",
        "\n",
        "  # Begin cluster\n",
        "  cl = parallel::makeCluster(detectCores()-1) # here you specify the number of processors you want to use, if you dont know you can use detectCores() and ideally use that number minus one\n",
        "  #cl = parallel::makeCluster(3, outfile=\"D:/r_parallel_log.txt\") # if you use this you can see prints in the txt\n",
        "  registerDoParallel(cl)\n",
        "\n",
        "  # apply the model in parallel\n",
        "  # sometimes you need to specify in the package parameter (.packages) the name of package of the functions you are using\n",
        "  value = foreach(i=1:length(y)) %dopar% { # note the %dopar% here\n",
        "    require(raster)\n",
        "    return(as.numeric(na.omit(mask(crop(x, y[i,]), y[i,])[])))\n",
        "  }\n",
        "\n",
        "  return(value)\n",
        "}\n",
        "\n",
        "# function to get the sp with the extent of a raster object\n",
        "rasext_to_sp = function(x) {\n",
        "  y = as(extent(x), \"SpatialPolygons\")\n",
        "  crs(y) = crs(x)\n",
        "  return(y)\n",
        "}\n",
        "\n",
        "# convert raster to vector using gdal_polygonize\n",
        "# this version accept the python path and poligonizer path separetely\n",
        "polygonizer_v2 <- function(x, outshape=NULL, pypath=NULL, polipath = NULL, readpoly=TRUE,\n",
        "                           fillholes=FALSE, aggregate=FALSE,\n",
        "                           quietish=TRUE) {\n",
        "  # x: an R Raster layer, or the file path to a raster file recognised by GDAL\n",
        "  # outshape: the path to the output shapefile (if NULL, a temporary file will\n",
        "  #           be created)\n",
        "  # pypath: the path to gdal_polygonize.py or OSGeo4W.bat (if NULL, the function\n",
        "  #         will attempt to determine the location)\n",
        "  # readpoly: should the polygon shapefile be read back into R, and returned by\n",
        "  #           this function? (logical)\n",
        "  # fillholes: should holes be deleted (i.e., their area added to the containing\n",
        "  #            polygon)\n",
        "  # aggregate: should polygons be aggregated by their associated raster value?\n",
        "  # quietish: should (some) messages be suppressed? (logical)\n",
        "  if (isTRUE(readpoly) || isTRUE(fillholes)) require(rgdal)\n",
        "\n",
        "  #cmd <- Sys.which(paste0(pypath, '\\\\OSGeo4W.bat'))\n",
        "  cmd = pypath\n",
        "  if (is.null(pypath) | is.null(polipath)) {\n",
        "    stop(\"Could not find gdal_polygonize.py or OSGeo4W on your system.\")\n",
        "  }\n",
        "  if (!is.null(outshape)) {\n",
        "    outshape <- sub('\\\\.shp$', '', outshape)\n",
        "    f.exists <- file.exists(paste(outshape, c('shp', 'shx', 'dbf'), sep='.'))\n",
        "    if (any(f.exists))\n",
        "      stop(sprintf('File already exists: %s',\n",
        "                   toString(paste(outshape, c('shp', 'shx', 'dbf'),\n",
        "                                  sep='.')[f.exists])), call.=FALSE)\n",
        "  } else outshape <- tempfile()\n",
        "  if (is(x, 'Raster')) {\n",
        "    require(raster)\n",
        "    writeRaster(x, {f <- tempfile(fileext='.tif')})\n",
        "    rastpath <- normalizePath(f)\n",
        "  } else if (is.character(x)) {\n",
        "    rastpath <- normalizePath(x)\n",
        "  } else stop('x must be a file path (character string), or a Raster object.')\n",
        "\n",
        "  # system2(cmd, args=(\n",
        "  #   sprintf('\"%s\" \"%s\" %s -f \"ESRI Shapefile\" \"%s.shp\"',\n",
        "  #           pypath, rastpath, ifelse(quietish, '-q ', ''), outshape)))\n",
        "  system2(cmd, sprintf('\"%s\" \"%s\" %s -f \"ESRI Shapefile\" \"%s.shp\"',\n",
        "                       polipath, rastpath, ifelse(quietish, '-q ', ''), outshape))\n",
        "\n",
        "  if(isTRUE(aggregate)||isTRUE(readpoly)||isTRUE(fillholes)) {\n",
        "    shp <- readOGR(dirname(outshape), layer=basename(outshape),\n",
        "                   verbose=!quietish)\n",
        "  } else return(NULL)\n",
        "\n",
        "  if (isTRUE(fillholes)) {\n",
        "    poly_noholes <- lapply(shp@polygons, function(x) {\n",
        "      Filter(function(p) p@ringDir==1, x@Polygons)[[1]]\n",
        "    })\n",
        "    pp <- SpatialPolygons(mapply(function(x, id) {\n",
        "      list(Polygons(list(x), ID=id))\n",
        "    }, poly_noholes, row.names(shp)), proj4string=CRS(proj4string(shp)))\n",
        "    shp <- SpatialPolygonsDataFrame(pp, shp@data)\n",
        "    if(isTRUE(aggregate)) shp <- aggregate(shp, names(shp))\n",
        "    writeOGR(shp, dirname(outshape), basename(outshape),\n",
        "             'ESRI Shapefile', overwrite=TRUE)\n",
        "  }\n",
        "  if(isTRUE(aggregate) & !isTRUE(fillholes)) {\n",
        "    shp <- aggregate(shp, names(shp))\n",
        "    writeOGR(shp, dirname(outshape), basename(outshape),\n",
        "             'ESRI Shapefile', overwrite=TRUE)\n",
        "  }\n",
        "  ifelse(isTRUE(readpoly), return(shp), return(NULL))\n",
        "}\n",
        "\n",
        "# function from spatial.tools (does not work in R4.0.2 yet so we copied from previous version)\n",
        "modify_raster_margins = function (x, extent_delta = c(0, 0, 0, 0), value = NA)\n",
        "{\n",
        "  x_extents <- extent(x)\n",
        "  res_x <- res(x)\n",
        "  x_modified <- x\n",
        "  if (any(extent_delta < 0)) {\n",
        "    ul_mod <- extent_delta[c(1, 3)] * res_x\n",
        "    ul_mod[ul_mod > 0] <- 0\n",
        "    lr_mod <- extent_delta[c(2, 4)] * res_x\n",
        "    lr_mod[lr_mod > 0] <- 0\n",
        "    crop_extent <- c(x_extents@xmin, x_extents@xmax, x_extents@ymin,\n",
        "                     x_extents@ymax)\n",
        "    crop_extent[c(1, 3)] <- crop_extent[c(1, 3)] - ul_mod\n",
        "    crop_extent[c(2, 4)] <- crop_extent[c(2, 4)] + lr_mod\n",
        "    x_modified <- crop(x_modified, crop_extent)\n",
        "  }\n",
        "  if (any(extent_delta > 0)) {\n",
        "    ul_mod <- extent_delta[c(1, 3)] * res_x\n",
        "    ul_mod[ul_mod < 0] <- 0\n",
        "    lr_mod <- extent_delta[c(2, 4)] * res_x\n",
        "    lr_mod[lr_mod < 0] <- 0\n",
        "    extend_extent <- c(x_extents@xmin, x_extents@xmax, x_extents@ymin,\n",
        "                       x_extents@ymax)\n",
        "    extend_extent[c(1, 3)] <- extend_extent[c(1, 3)] - ul_mod\n",
        "    extend_extent[c(2, 4)] <- extend_extent[c(2, 4)] + lr_mod\n",
        "    x_modified <- extend(x_modified, extend_extent, value = value)\n",
        "  }\n",
        "  return(x_modified)\n",
        "}\n",
        "\n",
        "# split the extent of a sp object\n",
        "split_extent_gdal = function(x, block_size = 1000, na_rm = T, remove_all_zero = T, gdal_path = NULL) {\n",
        "  if (is.null(gdal_path)) stop(\"Missing GDAL path.\")\n",
        "  # x = LIDAR_ANA_2017\n",
        "  # x = LIDAR_ST1_2016\n",
        "  x_ext = extent(x)\n",
        "\n",
        "  # create a temporary raster within the extent with block_size as pixel size\n",
        "  #n_x = ceiling(abs((x_ext[2] - x_ext[1])) / block_size)\n",
        "  n_y = abs((x_ext[4] - x_ext[3])) / block_size\n",
        "  # adjust extent to fit the cells\n",
        "  x_ext_mod = x_ext\n",
        "  x_ext_mod[4] = x_ext_mod[4] + ((ceiling(n_y) - n_y) * block_size)\n",
        "  #\n",
        "  r = raster(x_ext_mod, crs = crs(x), resolution = block_size)\n",
        "  r[] = NA\n",
        "  #\n",
        "  #plot(extend(extent(r),100), asp=1)\n",
        "  #plot(r, add=T, col=\"red\")\n",
        "  fname = paste0(tempfile(), \".tif\")\n",
        "  writeRaster(r, filename = fname, overwrite=T)\n",
        "\n",
        "  if (inMemory(x)) {\n",
        "    fname_x = paste0(tempfile(), \"_x.tif\")\n",
        "    writeRaster(x, filename = fname_x)\n",
        "    x = raster(fname_x)\n",
        "  }\n",
        "\n",
        "  # calculate the average of x inside the pixels\n",
        "  #gdal_path = \"C:\\\\GDAL_64\\\\\"\n",
        "  # command\n",
        "  # gdalwarp = paste(paste0(gdal_path,\"gdalwarp\")\n",
        "  #                  #,\"-r average -wm 9999\"\n",
        "  #                  ,\"-r average -wm 2047\"\n",
        "  #                  ,x@file@name\n",
        "  #                  ,fname\n",
        "  # )\n",
        "  # system(gdalwarp)\n",
        "  gdalUtils::gdalwarp(srcfile = x@file@name, dstfile = fname)\n",
        "\n",
        "  # load\n",
        "  r2 = raster(fname)\n",
        "  #plot(r2)\n",
        "\n",
        "  # convert raster to polygons - only those with values\n",
        "  r2_pol = rasterToPolygons(r2, dissolve=F, na.rm=na_rm)\n",
        "  # plot(r2_pol)\n",
        "\n",
        "  # exclude all zero\n",
        "  if (remove_all_zero) {\n",
        "    idx = which(r2_pol@data[]==0)\n",
        "    if (length(idx) > 0) r2_pol = r2_pol[-idx,]\n",
        "  }\n",
        "\n",
        "  # create extents\n",
        "  ext_list = list()\n",
        "  i=1\n",
        "  for (i in 1:length(r2_pol)) {\n",
        "    ext_list[[i]] = extent(r2_pol[i,])\n",
        "  }\n",
        "\n",
        "  unlink(fname)\n",
        "\n",
        "  return(ext_list)\n",
        "}\n",
        "#\n",
        "\n",
        "# split the extent of a sp object\n",
        "split_extent_gdal_bottom = function(x, block_size = 1000, na_rm = T, remove_all_zero = T, gdal_path = NULL) {\n",
        "  if (is.null(gdal_path)) stop(\"Missing GDAL path.\")\n",
        "  # x = LIDAR_ANA_2017\n",
        "  # x = LIDAR_ST1_2016\n",
        "  x_ext = extent(x)\n",
        "\n",
        "  # create a temporary raster within the extent with block_size as pixel size\n",
        "  #n_x = ceiling(abs((x_ext[2] - x_ext[1])) / block_size)\n",
        "  n_y = abs((x_ext[4] - x_ext[3])) / block_size\n",
        "  # adjust extent to fit the cells\n",
        "  x_ext_mod = x_ext\n",
        "  #x_ext_mod[4] = x_ext_mod[4] + ((ceiling(n_y) - n_y) * block_size)\n",
        "  x_ext_mod[3] = x_ext_mod[3] - ((ceiling(n_y) - n_y) * block_size)\n",
        "  #\n",
        "  r = raster(x_ext_mod, crs = crs(x), resolution = block_size)\n",
        "  r[] = NA\n",
        "  #\n",
        "  #plot(extend(extent(r),100), asp=1)\n",
        "  #plot(r, add=T, col=\"red\")\n",
        "  fname = paste0(tempfile(), \".tif\")\n",
        "  writeRaster(r, filename = fname, overwrite=T)\n",
        "\n",
        "  if (inMemory(x)) {\n",
        "    fname_x = paste0(tempfile(), \"_x.tif\")\n",
        "    writeRaster(x, filename = fname_x)\n",
        "    x = raster(fname_x)\n",
        "  }\n",
        "\n",
        "  # calculate the average of x inside the pixels\n",
        "  #gdal_path = \"C:\\\\GDAL_64\\\\\"\n",
        "  # command\n",
        "  gdalwarp = paste(paste0(gdal_path,\"gdalwarp\")\n",
        "                   #,\"-r average -wm 9999\"\n",
        "                   ,\"-r average -wm 2047\"\n",
        "                   ,x@file@name\n",
        "                   ,fname\n",
        "  )\n",
        "  system(gdalwarp)\n",
        "\n",
        "  # load\n",
        "  r2 = raster(fname)\n",
        "  #plot(r2)\n",
        "\n",
        "  # convert raster to polygons - only those with values\n",
        "  r2_pol = rasterToPolygons(r2, dissolve=F, na.rm=na_rm)\n",
        "  # plot(r2_pol)\n",
        "\n",
        "  # exclude all zero\n",
        "  if (remove_all_zero) {\n",
        "    idx = which(r2_pol@data[]==0)\n",
        "    if (length(idx) > 0) r2_pol = r2_pol[-idx,]\n",
        "  }\n",
        "\n",
        "  # create extents\n",
        "  ext_list = list()\n",
        "  i=1\n",
        "  for (i in 1:length(r2_pol)) {\n",
        "    ext_list[[i]] = extent(r2_pol[i,])\n",
        "  }\n",
        "\n",
        "  unlink(fname)\n",
        "\n",
        "  return(ext_list)\n",
        "}\n",
        "#\n",
        "\n",
        "# function to remove the last 4 digits of a string (usually the extension e.g. \".tif\") and substitute it for another string\n",
        "sub_extension = function (x, y) {\n",
        "  return(paste0(substr(x, 1, nchar(x)-4), y))\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ImjX1q9JtRHd"
      },
      "outputs": [],
      "source": [
        "# split the extent of a sp object\n",
        "split_extent_gdal_fixed = function(x, block_size = 1000, na_rm = T, remove_all_zero = T, gdal_path = NULL) {\n",
        "  if (is.null(gdal_path)) stop(\"Missing GDAL path.\")\n",
        "  # x = LIDAR_ANA_2017\n",
        "  # x = LIDAR_ST1_2016\n",
        "  x_ext = extent(x)\n",
        "\n",
        "  # create a temporary raster within the extent with block_size as pixel size\n",
        "  #n_x = ceiling(abs((x_ext[2] - x_ext[1])) / block_size)\n",
        "  n_y = abs((x_ext[4] - x_ext[3])) / block_size\n",
        "  # adjust extent to fit the cells\n",
        "  x_ext_mod = x_ext\n",
        "  #x_ext_mod[4] = x_ext_mod[4] + ((ceiling(n_y) - n_y) * block_size)\n",
        "  #x_ext_mod[3] = x_ext_mod[3] - ((ceiling(n_y) - n_y) * block_size)\n",
        "  #\n",
        "  r = raster(x_ext_mod, crs = crs(x), resolution = block_size)\n",
        "  r[] = NA\n",
        "  #\n",
        "  #plot(extend(extent(r),100), asp=1)\n",
        "  #plot(r, add=T, col=\"red\")\n",
        "  fname = paste0(tempfile(), \".tif\")\n",
        "  writeRaster(r, filename = fname, overwrite=T)\n",
        "\n",
        "  if (inMemory(x)) {\n",
        "    fname_x = paste0(tempfile(), \"_x.tif\")\n",
        "    writeRaster(x, filename = fname_x)\n",
        "    x = raster(fname_x)\n",
        "  }\n",
        "\n",
        "  # calculate the average of x inside the pixels\n",
        "  #gdal_path = \"C:\\\\GDAL_64\\\\\"\n",
        "  # command\n",
        "  gdalwarp = paste(paste0(gdal_path,\"gdalwarp\")\n",
        "                   #,\"-r average -wm 9999\"\n",
        "                   ,\"-r average -wm 2047\"\n",
        "                   ,x@file@name\n",
        "                   ,fname\n",
        "  )\n",
        "  system(gdalwarp)\n",
        "\n",
        "  # load\n",
        "  r2 = raster(fname)\n",
        "  #plot(r2)\n",
        "\n",
        "  # convert raster to polygons - only those with values\n",
        "  r2_pol = rasterToPolygons(r2, dissolve=F, na.rm=na_rm)\n",
        "  # plot(r2_pol)\n",
        "\n",
        "  # exclude all zero\n",
        "  if (remove_all_zero) {\n",
        "    idx = which(r2_pol@data[]==0)\n",
        "    if (length(idx) > 0) r2_pol = r2_pol[-idx,]\n",
        "  }\n",
        "\n",
        "  # create extents\n",
        "  ext_list = list()\n",
        "  i=1\n",
        "  for (i in 1:length(r2_pol)) {\n",
        "    ext_list[[i]] = extent(r2_pol[i,])\n",
        "  }\n",
        "\n",
        "  unlink(fname)\n",
        "\n",
        "  return(ext_list)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E79ait43tYS_"
      },
      "outputs": [],
      "source": [
        "# 1) load reference data -----------------------------------------------------\n",
        "\n",
        "# list shapefiles\n",
        "shp_list = list.files(\"4_shapes/Amostras.shp\", pattern = \".shp$\", full.names=TRUE)\n",
        "\n",
        "# load reference data\n",
        "ref_data = list()\n",
        "for (i in 1:length(shp_list)) {\n",
        "  ref_data[[i]] = readOGR(shp_list[i])\n",
        "}\n",
        "\n",
        "# # reproject to the same projection of the satellite data\n",
        "# ref_data = spTransform(ref_data, crs(\"+proj=utm +zone=19 +south +datum=WGS84 +units=m +no_defs\"))\n",
        "\n",
        "# visualization\n",
        "if (FALSE) {\n",
        "  # simple plot\n",
        "  plot(ref_data)\n",
        "\n",
        "  # visualize it over google map\n",
        "  map = leaflet() %>%\n",
        "    #addTiles() %>%\n",
        "    addTiles(urlTemplate = \"https://mts1.google.com/vt/lyrs=s&hl=en&src=app&x={x}&y={y}&z={z}&s=G\", attribution = 'Google') %>%\n",
        "    addPolygons(data = ref_data)\n",
        "  map ## show the map\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa1oTRYnN_UL"
      },
      "outputs": [],
      "source": [
        "# 2) define images to use and visualize ----------------------------------------------------\n",
        "\n",
        "# list images\n",
        "img_list = list.files(\"6_sampling1_prediction_singledate/pred_input/image\", pattern = \".JPG$\", full.names=TRUE)\n",
        "\n",
        "# open the rasters\n",
        "ras_list = list()\n",
        "for (i in 1:length(img_list)) {\n",
        "  ras_list[[i]] = stack(img_list[i])\n",
        "}\n",
        "\n",
        "# remove 2021 image\n",
        "# img_list = img_list[1:3]\n",
        "\n",
        "# function to correct y values collected at QGIS with flipped coordinates\n",
        "# v2 - also solves holes\n",
        "adjust_y_shp = function(shp, img) {\n",
        "  i=1\n",
        "  for (i in 1:length(shp@polygons)) {\n",
        "    for (j in 1:length(shp@polygons[[i]]@Polygons)) {\n",
        "      shp@polygons[[i]]@Polygons[[j]]@coords[,2] = (dim(img[[1]])[1] + shp@polygons[[i]]@Polygons[[j]]@coords[,2])\n",
        "    }\n",
        "  }\n",
        "  return(shp)\n",
        "}\n",
        "\n",
        "# adjust all shps\n",
        "i=1\n",
        "for (i in 1:length(img_list)) {\n",
        "  ref_data[i] = adjust_y_shp(ref_data[[i]], ras_list[[i]])\n",
        "}\n",
        "\n",
        "# name to add for this experiment\n",
        "exp_str = \"palms\"\n",
        "\n",
        "# about the data\n",
        "factor_image = 255\n",
        "data_n_layers = 3\n",
        "bands_to_use = 3\n",
        "add_border = TRUE\n",
        "\n",
        "# visualize the images\n",
        "if (FALSE) {\n",
        "  s2 = stack(img_list)\n",
        "  plot(s2[[5]])\n",
        "\n",
        "  # for singledate\n",
        "  mapview(s2[[5]]) + field_data # ndvi\n",
        "  mapview(s2[[4]]) + field_data # nir\n",
        "\n",
        "  # for multidate\n",
        "  mapview(s2[[4]]) + field_data # Feb 2020 NDVI\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkUGcAT0gdDO"
      },
      "outputs": [],
      "source": [
        "# 3) deep learning create grid of patches -----------------------------------------------\n",
        "\n",
        "# function to create a grid based on a image x and y pixels of patch size\n",
        "create_grid_flipy = function(x, y = 128, plot_grid=FALSE) {\n",
        "\n",
        "  # create a temporary raster within the extent with block_size as pixel size\n",
        "  # block_size is the size of each tile in meters\n",
        "  block_size = y*res(x)[1]\n",
        "\n",
        "  #\n",
        "  x_ext = extent(x)\n",
        "\n",
        "  # get the number of cells in the y axis\n",
        "  #n_x = ceiling(abs((x_ext[2] - x_ext[1])) / block_size)\n",
        "  n_y = abs((x_ext[4] - x_ext[3])) / block_size\n",
        "  # adjust extent to fit the cells\n",
        "  x_ext_mod = x_ext\n",
        "  x_ext_mod[4] = x_ext_mod[4] + ((ceiling(n_y) - n_y) * block_size)\n",
        "\n",
        "  # create a raster with this adjusted extent which resolution is our patch size\n",
        "  r = raster(x_ext_mod, crs = crs(x), resolution = block_size)\n",
        "  r[] = 1\n",
        "\n",
        "  # convert raster pixels to polygons\n",
        "  r_pol = rasterToPolygons(r, dissolve=F)#, na.rm=na_rm)\n",
        "  r_pol$id = 1:length(r_pol)\n",
        "\n",
        "  # visualize\n",
        "  if (plot_grid) {\n",
        "    x11()\n",
        "    plot(x[[1]])\n",
        "    plot(r_pol,add=T)\n",
        "  }\n",
        "}\n",
        "\n",
        "# Create grids\n",
        "i=1\n",
        "for (i in 1:length(img_list)) {\n",
        "  create_grid_flipy(x = raster(img_list[i]), y = 256)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1E4Eh_ACge1g"
      },
      "outputs": [],
      "source": [
        "# 4) deep learning sampling ----------------------------------------------------------------\n",
        "## the aim of this part is to crop the image into patches that we just created for the locations that have samples\n",
        "## Note: a limitation to the current code we are using is that it only takes 4 bands\n",
        "## this is because the use of the PNG file, need to adjust the code to use tif\n",
        "## so we can use more bands, this is why we just get 4 bands in the [[]] here\n",
        "## this takes ~15min\n",
        "\n",
        "# loop through imgs\n",
        "i_img = 1\n",
        "for (i_img in 1:length(img_list)) {\n",
        "\n",
        "  # load files\n",
        "  img = stack(img_list[i_img])[[bands_to_use]]\n",
        "  samples = st_as_sf(ref_data[[i_img]])\n",
        "  grid = st_read(list.files(\"3_grid\", pattern = sub_extension(basename(img_list[i_img]),\".shp\"), full.names = T))\n",
        "  #grid = st_set_crs(grid, st_crs(samples))\n",
        "  #grid = st_set_crs(grid, \"unknown\")\n",
        "  samples = st_set_crs(samples, \"unknown\")\n",
        "\n",
        "  # visualize\n",
        "  if (FALSE) {\n",
        "    plot(img[[1]])\n",
        "    plot(st_geometry(samples),add=T)\n",
        "    plot(st_geometry(grid),add=T)\n",
        "  }\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XPNCjrvW7XGr"
      },
      "outputs": [],
      "source": [
        "  # # reproject samples to the same CRS of the img and grid\n",
        "  # samples = st_transform(samples, as.character(crs(grid)))\n",
        "  #\n",
        "\n",
        "  # create other folders\n",
        "  dir.create(paste0(training_data_dir, \"/input/image\"), showWarnings = F, recursive=T)\n",
        "  dir.create(paste0(training_data_dir, \"/input/class\"), showWarnings = F, recursive=T)\n",
        "\n",
        "  # rasterize the samples\n",
        "  library(fasterize)\n",
        "  #samples_raster = fasterize(samples, img[[1]])\n",
        "  #samples_raster[is.na(samples_raster)]=0\n",
        "\n",
        "  # visualize\n",
        "  # plot(samples_raster)\n",
        "\n",
        "  # determine which grid id has samples\n",
        "  grid_intersection = st_intersection(grid, st_buffer(samples,0), byid=T)\n",
        "  #grid_intersection = grid\n",
        "\n",
        "  # next if empty\n",
        "  if (length(grid_intersection$id) == 0) next\n",
        "\n",
        "  # libraries needed\n",
        "  p_load(parallel, doParallel, foreach)\n",
        "\n",
        "  # Begin cluster\n",
        "  cl = parallel::makeCluster(no_cores) # here you specify the number of processors you want to use, if you dont know you can use detectCores() and ideally use that number minus one\n",
        "  #cl = parallel::makeCluster(3, outfile=\"D:/r_parallel_log.txt\") # if you use this you can see prints in the txt\n",
        "  registerDoParallel(cl)\n",
        "\n",
        "  # for each extent\n",
        "  i=1\n",
        "  #foreach(i = 1:1500, .inorder=F, .errorhandling='remove') %dopar% { # test just a few files\n",
        "  foreach(i = 1:length(grid_intersection$id), .inorder=F, .errorhandling='remove') %dopar% {\n",
        "    require(raster)\n",
        "    require(png)\n",
        "    require(rgeos)\n",
        "    require(sf)\n",
        "    require(fasterize)\n",
        "\n",
        "    # function to get the sp with the extent of a raster object\n",
        "    rasext_to_sp = function(x) {\n",
        "      y = as(extent(x), \"SpatialPolygons\")\n",
        "      crs(y) = crs(x)\n",
        "      return(y)\n",
        "    }\n",
        "\n",
        "    # check if there is intersection\n",
        "    if (!gIntersects(rasext_to_sp(img), as_Spatial(grid_intersection[i,]))) return(NA)\n",
        "\n",
        "    # crop img and mask\n",
        "    img_tmp = crop(img, grid[grid_intersection$id[i],])\n",
        "    plot(img_tmp)\n",
        "    class_tmp = fasterize(samples, img_tmp[[1]])\n",
        "    class_tmp[is.na(class_tmp)]=0\n",
        "    plot(class_tmp)\n",
        "\n",
        "    # define class presence or absence\n",
        "    class_presence = \"NOO\"\n",
        "    if(1 %in% getValues(class_tmp[[1]])) class_presence=\"YES\"\n",
        "    print(class_presence)\n",
        "\n",
        "    # row and columns are the same & img does not have NA values\n",
        "    if ((dim(class_tmp)[1] == dim(class_tmp)[2]) & !any(is.na(img_tmp[]))) {\n",
        "\n",
        "      # visualization\n",
        "      if (FALSE) {\n",
        "        #if (class_presence == \"YES\") {\n",
        "\n",
        "        plot(class_tmp); plot(img_tmp)\n",
        "        plot(stack(img_tmp, class_tmp), main = paste(\"layer:\",i))\n",
        "\n",
        "        # plot(extent(img_tmp),asp=1)\n",
        "        # plotRGB(img_tmp, r=1, g=2, b=3, add=T, stretch=\"lin\")\n",
        "        # plot(extent(img_tmp),asp=1)\n",
        "        # plot(class_tmp,add=T)\n",
        "      }\n",
        "\n",
        "      # save if YES - only if it has samples\n",
        "      #if (class_presence == \"YES\") {\n",
        "      if (class_presence == \"YES\" | TRUE) {\n",
        "\n",
        "        if (nlayers(img_tmp) == 1) {\n",
        "          # we divide by the scale of data\n",
        "          # for 1 layer\n",
        "          img_tmp2 = matrix(img_tmp, ncol = ncol(img_tmp), byrow = T) / factor_image\n",
        "        } else {\n",
        "\n",
        "          # for more layers\n",
        "          #img_tmp = stack(crop(img, grid[i,]) ,crop(img, grid[i,])*2 ,crop(img, grid[i,])*3,crop(img, grid[i,])*4 )\n",
        "          img_tmp2 = array(img_tmp, c(nrow(img_tmp), ncol(img_tmp), nlayers(img))) / factor_image\n",
        "          img_tmp2 = aperm(img_tmp2, c(2,1,3))\n",
        "        }\n",
        "\n",
        "        #\n",
        "        png::writePNG(img_tmp2, paste0(training_data_dir,\"\\\\input\\\\image\\\\img_\",class_presence,\"_\",sprintf(\"%05.0f\",i),\"_\",names(img),\".png\"))\n",
        "\n",
        "        #\n",
        "        class_tmp = matrix(class_tmp, ncol = ncol(class_tmp), byrow = T)\n",
        "        png::writePNG(class_tmp, paste0(training_data_dir,\"\\\\input\\\\class\\\\cla_\",class_presence,\"_\",sprintf(\"%05.0f\",i),\"_\",names(img),\".png\"))\n",
        "\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "    print(paste0(i,ifelse(class_presence==\"YES\", \" YES\",\"\")))\n",
        "  }\n",
        "\n",
        "  # finish cluster\n",
        "  stopCluster(cl)\n",
        "  print(i_img)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CXKvaO08grir"
      },
      "outputs": [],
      "source": [
        "# 4a) Visualize some patch samples --------------------------------------------\n",
        "\n",
        "# list samples\n",
        "list_img = list.files(img_dir, pattern = \"*.png\", full.names = TRUE)\n",
        "list_mask = list.files(class_dir, pattern = \"*.png\", full.names = TRUE)\n",
        "length(list_img)\n",
        "\n",
        "# plot\n",
        "# i=1\n",
        "i = sample(1:length(list_img), 1)\n",
        "r = stack(list_img[i], list_mask[i])\n",
        "plot(r)\n",
        "r\n",
        "print(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k83ShumcgwQj"
      },
      "outputs": [],
      "source": [
        "# 5) DL data visualization -----------------------------------------------\n",
        "\n",
        "# libraries we're going to need later\n",
        "# always put reticulate and use_python as the first packages to load, or you will not be able to choose the conda env/python\n",
        "p_load(reticulate)\n",
        "use_python(tensorflow_dir, required = T)\n",
        "p_load(keras, tfdatasets, tidyverse, rsample, magick)\n",
        "#py_config()\n",
        "\n",
        "# Quick visualization\n",
        "## it picks a random sample to show\n",
        "## if you run multiple times you will see different samples\n",
        "images <- tibble(\n",
        "  img = list.files(img_dir, pattern = \"*.png\", full.names = TRUE),\n",
        "  mask = list.files(class_dir, pattern = \"*.png\", full.names = TRUE)\n",
        ") %>%\n",
        "  sample_n(2) %>%\n",
        "  map(. %>% magick::image_read())\n",
        "#\n",
        "out <- magick::image_append(c(\n",
        "  magick::image_append(images$img, stack = TRUE),\n",
        "  magick::image_append(images$mask, stack = TRUE)\n",
        "))\n",
        "#\n",
        "plot(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHpB5nnygyn1"
      },
      "outputs": [],
      "source": [
        "# 6) deep learning training U-Net --------------------------------------------------\n",
        "\n",
        "# libraries we're going to need later\n",
        "# always put reticulate and use_python as the first packages to load, or you will not be able to choose the conda env/python\n",
        "p_load(reticulate)\n",
        "use_python(tensorflow_dir, required = T)\n",
        "p_load(keras, tfdatasets, tidyverse, rsample, magick)\n",
        "#py_config()\n",
        "\n",
        "# parameters\n",
        "epochs = 1000L\n",
        "#epochs = 15L\n",
        "batch_size = 32L\n",
        "lr_rate = 0.0001\n",
        "decay_rate = 0.0001\n",
        "img_dir = paste0(training_data_dir, \"./input/image\")\n",
        "class_dir = paste0(training_data_dir,\"./input/class\")\n",
        "\n",
        "\n",
        "# Quick visualization\n",
        "images <- tibble(\n",
        "  img = list.files(img_dir, pattern = \"*.png\", full.names = TRUE),\n",
        "  mask = list.files(class_dir, pattern = \"*.png\", full.names = TRUE)\n",
        ") %>%\n",
        "  sample_n(2) %>%\n",
        "  map(. %>% magick::image_read())\n",
        "#\n",
        "out <- magick::image_append(c(\n",
        "  magick::image_append(images$img, stack = TRUE),\n",
        "  magick::image_append(images$mask, stack = TRUE)\n",
        "))\n",
        "#\n",
        "plot(out)\n",
        "\n",
        "\n",
        "# load all data\n",
        "data_full <- tibble(\n",
        "  img = list.files(img_dir, pattern = \"*.png\", full.names = TRUE),\n",
        "  mask = list.files(class_dir, pattern = \"*.png\", full.names = TRUE)\n",
        ")\n",
        "\n",
        "# random sorting of the data\n",
        "set.seed(10)\n",
        "random_order=sample(1:dim(data_full)[1],dim(data_full)[1])\n",
        "data_full_reorder <- data_full[random_order,]\n",
        "\n",
        "# split the data between training and validation\n",
        "data_full_reorder <- initial_split(data_full_reorder, prop = 0.8)\n",
        "train_samples = length(data_full_reorder$in_id)\n",
        "train_fname = training(data_full_reorder)$img\n",
        "test_fname = testing(data_full_reorder)$img\n",
        "\n",
        "# # find the id on the name of imgs string\n",
        "# idx_last_underline = regexpr(\"\\\\_[^\\\\_]*$\", basename(test_fname)[1])[1]\n",
        "# ids_validation = as.numeric(substr(basename(test_fname), idx_last_underline + 1, nchar(basename(test_fname))[1] - 4))\n",
        "#\n",
        "# # find the id on the name of imgs string\n",
        "# idx_last_underline = regexpr(\"\\\\_[^\\\\_]*$\", basename(train_fname)[1])[1]\n",
        "# ids_train = as.numeric(substr(basename(train_fname), idx_last_underline + 1, nchar(basename(train_fname))[1] - 4))\n",
        "#\n",
        "# # get the polygons inside each block id\n",
        "# grid = readOGR(\"4_grid\\\\grid.shp\")\n",
        "# samples_patches_validation = grid[grid$id %in% ids_validation,]\n",
        "# samples_patches_train = grid[grid$id %in% ids_train,]\n",
        "# plot(samples_patches_train, main=\"train = black, validation = red\")\n",
        "# lines(samples_patches_validation, col=\"red\")\n",
        "#\n",
        "# # save\n",
        "# save(samples_patches_train, samples_patches_validation, file = paste0(\"deep_learning_patch_samples_\",exp_str,\".RData\"))\n",
        "#\n",
        "\n",
        "## the model\n",
        "\n",
        "# mixed precision\n",
        "tf$keras$mixed_precision$experimental$set_policy('mixed_float16')\n",
        "\n",
        "dice_coef <- custom_metric(\"custom\", function(y_true, y_pred, smooth = 1.0) {\n",
        "  y_true_f <- k_flatten(y_true)\n",
        "  y_pred_f <- k_flatten(y_pred)\n",
        "  intersection <- k_sum(y_true_f * y_pred_f)\n",
        "  result <- (2 * intersection + smooth) /\n",
        "    (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)\n",
        "  return(result)\n",
        "})\n",
        "\n",
        "bce_dice_loss <- function(y_true, y_pred) {\n",
        "  result <- loss_binary_crossentropy(y_true, y_pred) +\n",
        "    (1 - dice_coef(y_true, y_pred))\n",
        "  return(result)\n",
        "}\n",
        "\n",
        "#\n",
        "get_unet_128 <- function(input_shape = c(256, 256, data_n_layers),\n",
        "                         num_classes = 1) {\n",
        "\n",
        "  inputs <- layer_input(shape = input_shape)\n",
        "  # 128\n",
        "\n",
        "  down1 <- inputs %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  down1_pool <- down1 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 64\n",
        "\n",
        "  down2 <- down1_pool %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  down2_pool <- down2 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 32\n",
        "\n",
        "  down3 <- down2_pool %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  down3_pool <- down3 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 16\n",
        "\n",
        "  down4 <- down3_pool %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  down4_pool <- down4 %>%\n",
        "    layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "  # 8\n",
        "\n",
        "  center <- down4_pool %>%\n",
        "    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # center\n",
        "\n",
        "  up4 <- center %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down4, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 16\n",
        "\n",
        "  up3 <- up4 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down3, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 32\n",
        "\n",
        "  up2 <- up3 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down2, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 64\n",
        "\n",
        "  up1 <- up2 %>%\n",
        "    layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "    {layer_concatenate(inputs = list(down1, .), axis = 3)} %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\") %>%\n",
        "    layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "    layer_batch_normalization() %>%\n",
        "    layer_activation(\"relu\")\n",
        "  # 128\n",
        "\n",
        "  classify <- layer_conv_2d(up1,\n",
        "                            filters = num_classes,\n",
        "                            kernel_size = c(1, 1),\n",
        "                            dtype = 'float32', # mixed precision\n",
        "                            activation = \"sigmoid\")\n",
        "\n",
        "  model <- keras_model(\n",
        "    inputs = inputs,\n",
        "    outputs = classify\n",
        "  )\n",
        "\n",
        "  model %>% compile(\n",
        "    optimizer = optimizer_rmsprop(lr = lr_rate, decay = decay_rate),\n",
        "    loss = bce_dice_loss,\n",
        "    #loss = bce_dice_loss_flooding,\n",
        "    metrics = c(dice_coef)\n",
        "  )\n",
        "\n",
        "  return(model)\n",
        "}\n",
        "#\n",
        "model <- get_unet_128()\n",
        "\n",
        "## data augmentation\n",
        "\n",
        "# random brightness, contrast, hue\n",
        "random_bsh <- function(img) {\n",
        "  img <- img %>%\n",
        "    tf$image$random_brightness(max_delta = 0.2) %>%\n",
        "    tf$image$random_contrast(lower = 0.9, upper = 1.1) # %>%\n",
        "\n",
        "  # img <- tf$math$multiply(img,tf$random$uniform(shape =  shape(1L), minval = 0.4 ,maxval = 1.6 ,dtype = tf$float32))\n",
        "\n",
        "  # for RGB\n",
        "  # img_aug <- img[,,1:3]  %>%\n",
        "  #   tf$image$random_saturation(lower = 0.9, upper = 1.1) %>%\n",
        "  #   tf$image$random_hue(max_delta = 0.2) #%>%\n",
        "  #\n",
        "  # img <- tf$keras$backend$concatenate(\n",
        "  #   list(img_aug[,,1:3,drop=FALSE],img[,,4,drop=FALSE]), axis=-1L\n",
        "  # ) %>% tf$clip_by_value(0, 1)\n",
        "\n",
        "}\n",
        "random_flip_up_down <- function(x,y) { tf$cond(tf$less(y , 0.25) , function() tf$image$flip_up_down(x), function() x) }\n",
        "random_flip_left_right <- function(x,y) { tf$cond(tf$greater(y , 0.75), function() tf$image$flip_left_right(x), function() x) }\n",
        "\n",
        "\n",
        "# map data\n",
        "create_dataset <- function(data, train, batch_size = 8L, data_n_layers = 3) {\n",
        "\n",
        "  dataset <- data %>%\n",
        "    mutate(rot = ifelse(runif(dim(data)[1])>0.75,1,0)*runif(dim(data)[1],min=0,max=2*pi)) %>%\n",
        "    tensor_slices_dataset() %>%\n",
        "    dataset_map(~.x %>% list_modify(\n",
        "      img =  tf$image$decode_png(tf$io$read_file(.x$img),channels=data_n_layers),\n",
        "      mask = tf$image$decode_png(tf$io$read_file(.x$mask),channels=1)\n",
        "    )) %>%\n",
        "    dataset_map(~.x %>% list_modify(\n",
        "      img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n",
        "      #mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$uint8)\n",
        "      mask = tf$image$convert_image_dtype(.x$mask, dtype = tf$float32)\n",
        "    ))\n",
        "\n",
        "  # set rot variable to a random uniform value\n",
        "  dataset <- dataset %>% dataset_map(~.x %>% list_modify(\n",
        "    rot =tf$random$uniform(shape =  shape(1L), minval = 0 ,maxval = 1 ,dtype = tf$float32)\n",
        "  ))\n",
        "\n",
        "  # apply up/down and left/right flip conditioned by rot\n",
        "  dataset <- dataset  %>%\n",
        "    dataset_map(~.x %>% list_modify(\n",
        "      img = random_flip_up_down(x=.x$img,y=.x$rot),\n",
        "      mask = random_flip_up_down(x=.x$mask,y=.x$rot)\n",
        "    )) %>%\n",
        "    dataset_map(~.x %>% list_modify(\n",
        "      img = random_flip_left_right(x=.x$img,y=.x$rot),\n",
        "      mask = random_flip_left_right(x=.x$mask,y=.x$rot)\n",
        "    ))\n",
        "\n",
        "\n",
        "  # data augmentation performed on training set only\n",
        "  if (train) {\n",
        "    dataset <- dataset %>%\n",
        "      dataset_map(~.x %>% list_modify(\n",
        "        img = random_bsh(.x$img)\n",
        "      ))\n",
        "  }\n",
        "\n",
        "  # shuffling on training set only\n",
        "  if (train) {\n",
        "    dataset <- dataset %>%\n",
        "      #  dataset_shuffle(buffer_size = batch_size*128) #\n",
        "      dataset_shuffle(buffer_size = batch_size*128,seed=666,reshuffle_each_iteration=FALSE)\n",
        "  }\n",
        "\n",
        "  # train in batches; batch size might need to be adapted depending on\n",
        "  # available memory\n",
        "  dataset <- dataset %>%\n",
        "    dataset_batch(batch_size)\n",
        "\n",
        "  dataset %>%\n",
        "    # output needs to be unnamed\n",
        "    dataset_map(unname)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sjgsH3WFhFnj"
      },
      "outputs": [],
      "source": [
        "## Train\n",
        "\n",
        "# Training and test set creation now is just a matter of two function calls.\n",
        "training_dataset <- create_dataset(training(data_full_reorder), train = TRUE, data_n_layers = data_n_layers)\n",
        "validation_dataset <- create_dataset(testing(data_full_reorder), train = FALSE, data_n_layers = data_n_layers)\n",
        "\n",
        "# callbacks\n",
        "dir.create(paste0(training_data_dir, \"./epoch_history/\"), showWarnings=F)\n",
        "dir.create(paste0(training_data_dir, \"./weights/\"), showWarnings=F)\n",
        "dir.create(paste0(training_data_dir, \"./weights_r_save/\"), showWarnings=F)\n",
        "callbacks_list <- list(\n",
        "  callback_csv_logger(paste0(training_data_dir, \"./epoch_history/epoch_history.csv\"), separator = \";\", append = FALSE),\n",
        "  callback_model_checkpoint(filepath = paste0(training_data_dir, \"./weights/unet_tf2_{epoch:03d}_{val_custom:.4f}.h5\"),\n",
        "                            monitor = \"val_custom\",save_best_only = TRUE,\n",
        "                            save_weights_only = TRUE, mode = \"max\" ,save_freq = NULL)\n",
        ")\n",
        "\n",
        "## start training from a set of weights\n",
        "# load_model_weights_hdf5(model, \"./weights_r_save/unet_tf2_111_0.6083_noBSH.h5\") # example\n",
        "\n",
        "# train\n",
        "training_dataset <- dataset_repeat(training_dataset, count = epochs)\n",
        "fit_generator(model,training_dataset,validation_data = validation_dataset, workers = 1, steps_per_epoch = as.integer(train_samples / batch_size), epochs = epochs,callbacks = callbacks_list)\n",
        "\n",
        "\n",
        "# clear GPU\n",
        "tf$keras.backend$clear_session()\n",
        "py_gc <- import('gc')\n",
        "py_gc$collect()\n",
        "\n",
        "## visualize prediction\n",
        "\n",
        "if (FALSE) {\n",
        "\n",
        "  ## load saved weights\n",
        "  load_model_weights_hdf5(model, weights_fname) # sample5\n",
        "\n",
        "  ## test\n",
        "  #predictions <- predict(model, validation_dataset)\n",
        "  predictions = predict(model, create_dataset(data = testing(data_full_reorder), train = F, data_n_layers = data_n_layers))\n",
        "  dim(predictions)\n",
        "  i=sample(1:dim(predictions)[1], 1)\n",
        "  # i=10\n",
        "  par(mfrow = c(2,2))\n",
        "  plot(raster(testing(data_full_reorder)$img[i]), main=paste0(i, \"_img\"))\n",
        "  plot(raster(testing(data_full_reorder)$mask[i]), main = \"mask\")\n",
        "  plot(raster(predictions[i,,,]), main = \"Unet\")# > 0.5)\n",
        "  plot(raster(predictions[i,,,]) > 0.5, main = \"Unet\")\n",
        "\n",
        "\n",
        "\n",
        "  ## load saved weights\n",
        "  load_model_weights_hdf5(model, weights_fname) # sample5\n",
        "\n",
        "  # example of prediction on the validation data set\n",
        "  #predictions <- predict(model, validation_dataset)\n",
        "  # example of prediction on a batch of the the validation data set\n",
        "  batch <- validation_dataset %>% as_iterator() %>% iter_next()\n",
        "  predictions <- predict(model, batch)\n",
        "\n",
        "  # clear GPU\n",
        "  tf$keras.backend$clear_session()\n",
        "  py_gc <- import('gc')\n",
        "  py_gc$collect()\n",
        "\n",
        "  # visualize\n",
        "  images <- tibble(\n",
        "    image = batch[[1]] #%>% tf$image$central_crop(0.8)\n",
        "    %>% array_branch(1),\n",
        "    predicted_mask = predictions %>% array_branch(1),\n",
        "    mask = batch[[2]]  %>% array_branch(1)\n",
        "  ) %>%\n",
        "    sample_n(2) %>%\n",
        "    map_depth(2, function(x) {\n",
        "      as.raster(x[,,1]) %>% magick::image_read()\n",
        "    }) %>%\n",
        "    map(~do.call(c, .x))\n",
        "  #\n",
        "  out <- magick::image_append(c(\n",
        "    magick::image_append(images$mask, stack = TRUE),\n",
        "    magick::image_append(images$image, stack = TRUE),\n",
        "    magick::image_append(images$predicted_mask, stack = TRUE)\n",
        "  ))\n",
        "  plot(out)\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S5ScEYnvhNPp"
      },
      "outputs": [],
      "source": [
        "# 7) Prediction: load some parameters ------------------------------------------\n",
        "\n",
        "# libraries we're going to need later\n",
        "# always put reticulate and use_python as the first packages to load, or you will not be able to choose the conda env/python\n",
        "p_load(reticulate)\n",
        "use_python(tensorflow_dir, required = T)\n",
        "p_load(keras,\n",
        "       tfdatasets,\n",
        "       tidyverse,\n",
        "       rsample\n",
        ")\n",
        "#py_config()\n",
        "\n",
        "# gdal functions\n",
        "gdalbuildvrt = shQuote(shortPathName(normalizePath(file.path(gdal_path, \"gdalbuildvrt.exe\"))))\n",
        "gdal_translate = shQuote(shortPathName(normalizePath(file.path(gdal_path, \"gdal_translate.exe\"))))\n",
        "gdal_info = shQuote(shortPathName(normalizePath(file.path(gdal_path, \"gdalinfo.exe\"))))\n",
        "\n",
        "# test dir\n",
        "test_dir <- paste0(prediction_data_dir, \"./pred_input/\")\n",
        "\n",
        "\n",
        "# raster opts\n",
        "dir.create(paste0(prediction_data_dir, \"\\\\tmp\"), showWarnings = F)\n",
        "rasterOptions(tmpdir=\"tmp\")\n",
        "#rasterOptions(maxmemory = 5e+10)\n",
        "#rasterOptions(chunksize = 1e+09)\n",
        "\n",
        "\n",
        "\n",
        "# create dirs\n",
        "dir.create(paste0(prediction_data_dir, \"\\\\pred_input\"), showWarnings = F, recursive=T)\n",
        "dir.create(paste0(prediction_data_dir,\"\\\\pred_output\"), showWarnings = F)\n",
        "dir.create(paste0(prediction_data_dir,\"\\\\pred_mosaic\"), showWarnings = F, recursive=T)\n",
        "dir.create(paste0(prediction_data_dir,\"\\\\pred_vector\"), showWarnings = F, recursive=T)\n",
        "\n",
        "# clear some folders before starting\n",
        "unlink(list.files(paste0(prediction_data_dir, \"./pred_input/\"), full.names = TRUE))\n",
        "unlink(list.files(paste0(prediction_data_dir, \"./pred_output/\"), full.names = TRUE))\n",
        "unlink(list.files(paste0(prediction_data_dir, \"./tmp/\"), full.names = TRUE))\n",
        "\n",
        "\n",
        "\n",
        "# # load data\n",
        "# img_list = list.files(\"2_Images\\\\\", full.names=T, pattern = \".tif$\")\n",
        "# # 665-971, i = 3\n",
        "#\n",
        "# # filter\n",
        "# #img_list = img_list[1:3]\n",
        "\n",
        "# loop images\n",
        "i_img=1\n",
        "for (i_img in 1:length(img_list)) {\n",
        "\n",
        "\n",
        "  # jump iteration if file exists\n",
        "  if (file.exists(paste(prediction_data_dir, \"./pred_mosaic/\", result_fname, \"_\",sub_extension(basename(img_list[i_img]),\"\"),\".tif\",sep=\"\"))) {\n",
        "    print(\"next\")\n",
        "    next\n",
        "  }\n",
        "\n",
        "\n",
        "  # # load\n",
        "  # #predictor_data = stack(img_list)\n",
        "  # predictor_data_expand = stack(img_list[i_img])[[1:4]]\n",
        "  #\n",
        "  #\n",
        "  # # define the extents\n",
        "  # block_size = 512*res(predictor_data_expand)[1]\n",
        "  # #predictor_data_ext = split_extent_gdal_bottom(x = predictor_data_expand[[1]], block_size = block_size, na_rm = F, remove_all_zero = F, gdal_path = gdal_path)\n",
        "  # predictor_data_ext = split_extent_gdal_fixed(x = predictor_data_expand[[1]], block_size = block_size, na_rm = F, remove_all_zero = F, gdal_path = gdal_path)\n",
        "  # # predictor_data_ext = split_extent_gdal_border(x =predictor_data_expand, block_size = block_size,\n",
        "  # #                                               na_rm = F,\n",
        "  # #                                               remove_all_zero = F, gdal_path = gdal_path)\n",
        "  # length(predictor_data_ext)\n",
        "  #\n",
        "  # # visualize the patches\n",
        "  # plot(predictor_data_expand[[1]])\n",
        "  # for (i in 1:10) plot(predictor_data_ext[[i]], add=T, col=i)\n",
        "  # # for (i in 1:length(predictor_data_ext)) plot(predictor_data_ext[[i]], add=T, col=i)\n",
        "  #\n",
        "  # ## to fix borders, one possibility is to create tiles only for the borders of bottom and right\n",
        "  #\n",
        "  #\n",
        "  # # 32 meters, or 64 pixels (0.5 m)\n",
        "  # #vect_overlap = c(-32,32,-32,32)\n",
        "  # vect_overlap = c(-res(predictor_data_expand[[1]])[1]*64,res(predictor_data_expand[[1]])[1]*64,-res(predictor_data_expand[[1]])[1]*64, res(predictor_data_expand[[1]])[1]*64)\n",
        "  #\n",
        "\n",
        "\n",
        "\n",
        "  # prep border -------------------------------------------------------------\n",
        "\n",
        "  # set dir to place the files\n",
        "  tiles_dir = paste0(prediction_data_dir,\"\\\\tmp\\\\\")\n",
        "\n",
        "  # set pantif\n",
        "  pantif = paste0(tiles_dir, basename(img_list[i_img]))\n",
        "\n",
        "  # if output is JPG, change output to TIF to avoid errors later on\n",
        "  pantif = sub_extension(pantif, \".tif\")\n",
        "\n",
        "\n",
        "  #les images subset pour creer le bordure\n",
        "  panpredtiftmp=sub(\"PAN\",\"tmp\",pantif)\n",
        "  # sinon je fais haut bas gauche droite\n",
        "  pantop=sub(\"PAN\",\"tmp\",pantif)\n",
        "  pantop=sub(\"\\\\.tif\",\"\\\\_top.tif\",pantop)\n",
        "  panbot=sub(\"PAN\",\"tmp\",pantif)\n",
        "  panbot=sub(\"\\\\.tif\",\"\\\\_bot.tif\",panbot)\n",
        "  panlef=sub(\"PAN\",\"tmp\",pantif)\n",
        "  panlef=sub(\"\\\\.tif\",\"\\\\_lef.tif\",panlef)\n",
        "  panrig=sub(\"PAN\",\"tmp\",pantif)\n",
        "  panrig=sub(\"\\\\.tif\",\"\\\\_rig.tif\",panrig)\n",
        "\n",
        "  # les images finales centre + mirror des bordure\n",
        "  panpredtif=sub(\"PAN\",\"PANPRED\",pantif)\n",
        "\n",
        "  # extent of the main image\n",
        "  extn=extent(raster(img_list[i_img]))\n",
        "\n",
        "  # # apply factor to the whole image\n",
        "  # r = stack(img_list[i_img])\n",
        "  # r = r / factor_image\n",
        "  # img_name_scaled = paste0(tiles_dir, sub_extension(basename(img_list[i_img]),\"_scaled.tif\"))\n",
        "  # writeRaster(r, img_name_scaled, overwrite=T)\n",
        "\n",
        "  # get img name\n",
        "  img_name = img_list[i_img]\n",
        "\n",
        "  # change data type because data is not INT1U or INT2U\n",
        "  img_name_output = paste0(tiles_dir, sub_extension(basename(img_name),\"_INT2U.tif\"))\n",
        "  system.time({system(paste(gdal_translate, \"-ot UInt16 -co COMPRESS=DEFLATE\", img_name, img_name_output))})\n",
        "  img_name = img_name_output\n",
        "\n",
        "  # re-save to solve a problem with gdalvrt not recognizing the georeferencing\n",
        "  img_name_output = paste0(tiles_dir, sub_extension(basename(img_name),\"_geo.tif\"))\n",
        "  writeRaster(brick(img_name), filename = img_name_output, overwrite=T)\n",
        "  img_name = img_name_output\n",
        "\n",
        "  # border size\n",
        "  border_size = 64 # 0.125 * 512\n",
        "  #predict_tile_size = 512\n",
        "  predict_tile_size = 256\n",
        "\n",
        "  ## add border?\n",
        "  if (add_border) {\n",
        "\n",
        "\n",
        "    # save full image without the 5th band\n",
        "    #r = stack(img_list[i_img])[[1:4]]\n",
        "    if (length(bands_to_use) == 4) {\n",
        "      img_name_output = paste0(tiles_dir, sub_extension(basename(img_name),\"_scaled_colorinterp.tif\"))\n",
        "      #writeRaster(r, img_name, datatype=\"INT2U\", overwrite=T)\n",
        "      system.time({system(paste(gdal_translate, \"-b 1 -b 2 -b 3 -b 4 -colorinterp red,green,blue,alpha\", img_name, img_name_output))})\n",
        "      img_name = img_name_output\n",
        "\n",
        "    }\n",
        "\n",
        "    # system(paste(gdal_info, name_final_TIF))\n",
        "    # system(paste(gdal_info, pantoptmp))\n",
        "    # system(paste(gdal_info, pantop))\n",
        "    # system(paste(gdal_info, panbot))\n",
        "    # system(paste(gdal_info, img_name))\n",
        "    # system(paste(gdal_info, img_list[i_img]))\n",
        "\n",
        "\n",
        "    # get data type from original data\n",
        "    data_type = dataType(raster(img_name))\n",
        "\n",
        "\n",
        "    # get image dimension - this assume its a square\n",
        "    img_dim = dim(raster(img_name))\n",
        "    res_im = res(raster(img_name))[1]\n",
        "\n",
        "    ## if img is JPG convert to tif in the next step\n",
        "\n",
        "    # top\n",
        "    #extent_coord_top=paste(\"-a_ullr\",extent_coord@xmin,extent_coord@ymax,extent_coord@xmax,extent_coord@ymax + 64*res_im,sep=\" \")\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), \"-srcwin 0 1\", img_dim[2], border_size, img_name, pantop))})\n",
        "    rt=brick(pantop)\n",
        "    rt=flip(rt,\"y\")\n",
        "    extent(rt)=c(extn@xmin,extn@xmax,extn@ymax,extn@ymax + border_size*res_im)\n",
        "    pantoptmp=sub(\"_top\",\"_top_tmp\",pantop)\n",
        "    writeRaster(rt,filename = pantoptmp, datatype=data_type,overwrite=T)\n",
        "    #writeRaster(rt,filename = pantoptmp, overwrite=T)\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), pantoptmp, pantop))})\n",
        "\n",
        "    # bot\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), \"-srcwin 0\", img_dim[1]-border_size-1, img_dim[2], border_size, img_name, panbot))})\n",
        "    rb=brick(panbot)\n",
        "    rb=flip(rb,\"y\")\n",
        "    extent(rb)=c(extn@xmin,extn@xmax,extn@ymin - (border_size*res_im),extn@ymin)\n",
        "    panbottmp=sub(\"_bot\",\"_bot_tmp\",panbot)\n",
        "    writeRaster(rb,filename = panbottmp,datatype=data_type,overwrite=T)\n",
        "    #writeRaster(rb,filename = panbottmp, overwrite=T)\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), panbottmp, panbot))})\n",
        "\n",
        "    # 1st merge\n",
        "    #list_datamask=c( img_name,pantoptmp,panbottmp)\n",
        "    list_datamask=c( img_name,pantop,panbot)\n",
        "    list_datamask=paste(list_datamask,collapse =\" \")\n",
        "    name_final_vrt=sub(\".tif\",\"_tmp.vrt\",panpredtiftmp)\n",
        "    name_final_TIF=sub(\".tif\",\"_tmp.tif\",panpredtiftmp)\n",
        "    system(paste(gdalbuildvrt, name_final_vrt ,list_datamask ))\n",
        "    ## this code with no data 255 was introducing lots of noise\n",
        "    #system.time({system(paste(gdal_translate, \"-co COMPRESS=DEFLATE -a_nodata 255\", name_final_vrt, name_final_TIF))})\n",
        "    system.time({system(paste(gdal_translate, \"-co COMPRESS=DEFLATE\", name_final_vrt, name_final_TIF))})\n",
        "    # system.time({system(paste(\"C:/OSGeo4W64/OSGeo4W.bat\",\"C:/OSGeo4W64/bin/gdal_translate.exe\", \"-co COMPRESS=DEFLATE -a_nodata 255 --config GDAL_CACHEMAX 10000\", name_final_vrt, name_final_TIF))})\n",
        "\n",
        "    # right\n",
        "    extn=extent(raster(name_final_TIF))\n",
        "    system.time({system(paste(gdal_translate, \"-srcwin\", img_dim[2]-border_size-1, 0, border_size, img_dim[1]+border_size+border_size, name_final_TIF, panrig ))})\n",
        "    rr=brick(panrig)\n",
        "    rr=flip(rr,\"x\")\n",
        "    extent(rr)=c(extn@xmax,extn@xmax+(border_size*res_im),extn@ymin,extn@ymax)\n",
        "    panrigtmp=sub(\"_rig\",\"_rig_tmp\",panrig)\n",
        "    writeRaster(rr,filename = panrigtmp,datatype=data_type,overwrite=T)\n",
        "    #writeRaster(rr,filename = panrigtmp, overwrite=T)\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), panrigtmp, panrig))})\n",
        "\n",
        "    # left\n",
        "    extn=extent(raster(name_final_TIF))\n",
        "    system.time({system(paste(gdal_translate, \"-srcwin 1 0\", border_size, img_dim[1]+border_size+border_size, name_final_TIF, panlef ))})\n",
        "    rl=brick(panlef)\n",
        "    rl=flip(rl,\"x\")\n",
        "    extent(rl)=c(extn@xmin-(border_size*res_im),extn@xmin,extn@ymin,extn@ymax)\n",
        "    panleftmp=sub(\"_lef\",\"_lef_tmp\",panlef)\n",
        "    writeRaster(rl,filename = panleftmp,datatype=data_type,overwrite=T)\n",
        "    #writeRaster(rl,filename = panleftmp, overwrite=T)\n",
        "    system.time({system(paste(gdal_translate, ifelse(length(bands_to_use) == 4, \"-colorinterp red,green,blue,alpha\", \"\"), panleftmp, panlef))})\n",
        "\n",
        "    list_datamask=c( name_final_TIF,panrig,panlef)\n",
        "    list_datamask=paste(list_datamask,collapse =\" \")\n",
        "    name_final_vrt=sub(\"_tmp.tif\",\".vrt\",name_final_TIF)\n",
        "    name_final_TIF=sub(\"_tmp.tif\",\".tif\",name_final_TIF)\n",
        "    #name_final_vrt=sub(\"tmp\",\"PANPRED\",name_final_vrt)\n",
        "    #name_final_TIF=sub(\"tmp\",\"PANPRED\",name_final_TIF)\n",
        "    system(paste(gdalbuildvrt, name_final_vrt ,list_datamask ))\n",
        "    system.time({system(paste(gdal_translate, \"-co COMPRESS=DEFLATE\", name_final_vrt, name_final_TIF))})\n",
        "    # mesma coisa\n",
        "    # system.time({system(paste(\"C:/OSGeo4W64/OSGeo4W.bat\",\"C:/OSGeo4W64/bin/gdal_translate.exe\", \"-co COMPRESS=DEFLATE -a_nodata 255 --GDAL_CACHEMAX 10000\", name_final_vrt, name_final_TIF))})\n",
        "\n",
        "  } else {\n",
        "\n",
        "    # if no border, set the original filename as the name_final_TIF\n",
        "    name_final_TIF = img_name\n",
        "  }\n",
        "\n",
        "\n",
        "  # get data crs\n",
        "  data_crs = crs(raster(name_final_TIF))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  # new crop ----------------------------------------------------------------\n",
        "\n",
        "  # set dir to place the files\n",
        "  tiles_dir = paste0(prediction_data_dir,\"\\\\pred_input\\\\\")\n",
        "\n",
        "  # testing installing this\n",
        "  #https://gis.stackexchange.com/questions/259902/gdal-importerror-dll-load-failed-the-specified-module-could-not-be-found\n",
        "  #https://docs.microsoft.com/en-us/cpp/windows/latest-supported-vc-redist?view=msvc-160\n",
        "#  I got it working by installing the Microsoft Visual C++ Redistributable for Visual Studio 2015, 2017 and 2019, as indicated in the header of Gohlke's website. I'm on Windows 10 and I installed geopandas version 0.6.2 using conda and python 3.7.3, gdal version 3.0.2 and fiona version 1.8.11.\n",
        "\n",
        "\n",
        "  # then https://stackoverflow.com/questions/6009506/unable-to-install-python-and-gdal-dll-load-failed\n",
        "  # Yes, it works like a charm! Basically, it is VERY IMPORTANT, to match the compiler version of Python, the Python binding and the GDAL Core package. In m\n",
        "\n",
        "  # conda create -n gdal_env gdal\n",
        "\n",
        "  # create the patches with a border - TIF format\n",
        "  system.time({\n",
        "    #system(paste(paste0(py_path,\"python\"),paste0(gdal_py_path, \"/gdal_retile.py\"),\" -ps 640 640\",\"-overlap 128\",\"-tileIndex index.shp\",\"--config GDAL_CACHEMAX 2000\", \"-targetDir\", tiles_dir,  name_final_TIF, sep=\" \"))\n",
        "    system(paste(paste0(py_path,\"python\"),paste0(gdal_py_path, \"/gdal_retile.py\"),\" -ps\",predict_tile_size,predict_tile_size,\"-overlap\", border_size*2,\"-tileIndex index.shp\",\"--config GDAL_CACHEMAX 2000\", \"-targetDir\", tiles_dir,  name_final_TIF, sep=\" \"))\n",
        "  })\n",
        "  ## 3 min\n",
        "\n",
        "  # # create the patches with a border - PNG format\n",
        "  # # for PNG to work data must be either 8-bit (INT1U) or 16-bit unsigned (INT2U)\n",
        "  # system.time({\n",
        "  #   system(paste(paste0(py_path,\"python\"),paste0(gdal_py_path, \"/gdal_retile.py\"),\" -ps 640 640\",\"-overlap 128\",\"-of PNG\",\"-tileIndex index.shp\",\"--config GDAL_CACHEMAX 2000\", \"-targetDir\", tiles_dir, name_final_TIF, sep=\" \"))\n",
        "  # })\n",
        "  # ## 27 min\n",
        "\n",
        "  # r = raster(\"E:\\\\2_Vinicius_Geoglifo\\\\5_sampling1_prediction\\\\pred_input\\\\CBERS_4A_WPM_20200528_234_125_L4_BAND0_INT2U_053_091.tif\")\n",
        "  # r = raster(\"E:\\\\2_Vinicius_Geoglifo\\\\5_sampling1_prediction\\\\pred_input\\\\CBERS_4A_WPM_20200528_234_125_L4_BAND0_INT2U_053_091.png\")\n",
        "  # r\n",
        "  # plot(r)\n",
        "\n",
        "\n",
        "  # apply factor to png ------------------------------------------------------------\n",
        "\n",
        "  # clean the non square tifs - so we dont have problem later with predict\n",
        "  tif_list = list.files(tiles_dir, pattern = \".tif$\", full.names = T)\n",
        "  i=1\n",
        "  for (i in 1:length(tif_list)) {\n",
        "    r = raster(tif_list[i])\n",
        "    if (dim(r)[1] != dim(r)[2]) {\n",
        "      file.remove(tif_list[i])\n",
        "    }\n",
        "  }\n",
        "\n",
        "  system.time({\n",
        "    # list png files\n",
        "    png_list = list.files(tiles_dir, pattern = \".tif$\", full.names = T)\n",
        "    png_list\n",
        "\n",
        "    # libraries needed\n",
        "    p_load(parallel, doParallel, foreach)\n",
        "\n",
        "    # Begin cluster\n",
        "    cl = parallel::makeCluster(no_cores) # here you specify the number of processors you want to use, if you dont know you can use detectCores() and ideally use that number minus one\n",
        "    #cl = parallel::makeCluster(3, outfile=\"D:/r_parallel_log.txt\") # if you use this you can see prints in the txt\n",
        "    registerDoParallel(cl)\n",
        "\n",
        "    Sys.time()\n",
        "    # for each extent\n",
        "    i=1000\n",
        "    foreach(i = 1:length(png_list), .inorder=F, .errorhandling='remove') %dopar% {\n",
        "      require(raster)\n",
        "      require(png)\n",
        "\n",
        "      img_tmp = stack(png_list[i])\n",
        "\n",
        "      # check size\n",
        "      if (dim(img_tmp)[1] == dim(img_tmp)[2]) {\n",
        "\n",
        "        # we divide by the scale of data\n",
        "        if (nlayers(img_tmp) == 1) {\n",
        "          # for 1 layer\n",
        "          img_tmp2 = matrix(img_tmp, ncol = ncol(img_tmp), byrow = T) / factor_image\n",
        "\n",
        "        } else {\n",
        "\n",
        "          # for more layers\n",
        "          img_tmp2 = array(img_tmp, c(nrow(img_tmp), ncol(img_tmp), nlayers(img_tmp))) / factor_image\n",
        "          img_tmp2 = aperm(img_tmp2, c(2,1,3))\n",
        "\n",
        "        }\n",
        "        #png::writePNG(img_tmp2, png_list[i])\n",
        "        png::writePNG(img_tmp2, sub_extension(png_list[i],\".png\"))\n",
        "        print(i)\n",
        "\n",
        "      }\n",
        "\n",
        "    }\n",
        "\n",
        "    # finish cluster\n",
        "    stopCluster(cl)\n",
        "  }) ## 11 min\n",
        "\n",
        "  # clear tmp\n",
        "  unlink(list.files(paste0(prediction_data_dir,\"\\\\tmp\\\\\"),full.names = TRUE), recursive=FALSE )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANzXT5A4haU-"
      },
      "outputs": [],
      "source": [
        "# # 7a) crop image into patches -------------------------------------------------\n",
        "  #\n",
        "  #\n",
        "  # # libraries needed\n",
        "  # p_load(parallel, doParallel, foreach)\n",
        "  #\n",
        "  # # Begin cluster\n",
        "  # cl = parallel::makeCluster(no_cores) # here you specify the number of processors you want to use, if you dont know you can use detectCores() and ideally use that number minus one\n",
        "  # #cl = parallel::makeCluster(3, outfile=\"D:/r_parallel_log.txt\") # if you use this you can see prints in the txt\n",
        "  # registerDoParallel(cl)\n",
        "  #\n",
        "  # # for each extent\n",
        "  # i=1000\n",
        "  # foreach(i = 1:length(predictor_data_ext), .inorder=F, .errorhandling='remove') %dopar% {\n",
        "  #   require(raster)\n",
        "  #   require(png)\n",
        "  #\n",
        "  #   # crop img\n",
        "  #   #img_tmp = crop(predictor_data_expand_64, predictor_data_ext[[i]] + vect_overlap)\n",
        "  #   #img_tmp = crop(predictor_data_expand, predictor_data_ext[[i]] + vect_overlap)\n",
        "  #   img_tmp = crop(predictor_data_expand, predictor_data_ext[[i]]) # no borders\n",
        "  #\n",
        "  #   # change NA to 0\n",
        "  #   img_tmp[is.na(img_tmp),] = 0\n",
        "  #\n",
        "  #   # row and columns are the same\n",
        "  #   if (dim(img_tmp)[1] == dim(img_tmp)[2]) {\n",
        "  #\n",
        "  #     # we divide by the scale of data\n",
        "  #     if (nlayers(img_tmp) == 1) {\n",
        "  #       # for 1 layer\n",
        "  #       img_tmp2 = matrix(img_tmp, ncol = ncol(img_tmp), byrow = T) / factor_image\n",
        "  #\n",
        "  #     } else {\n",
        "  #\n",
        "  #       # for more layers\n",
        "  #       img_tmp2 = array(img_tmp, c(nrow(img_tmp), ncol(img_tmp), nlayers(img_tmp))) / factor_image\n",
        "  #       img_tmp2 = aperm(img_tmp2, c(2,1,3))\n",
        "  #\n",
        "  #     }\n",
        "  #\n",
        "  #     # save\n",
        "  #     png::writePNG(img_tmp2, paste0(prediction_data_dir,\"\\\\pred_input\\\\img\",\"_\",sprintf(\"%05.0f\",i),\".png\"))\n",
        "  #\n",
        "  #\n",
        "  #   }\n",
        "  #\n",
        "  # }\n",
        "  #\n",
        "  # # finish cluster\n",
        "  # stopCluster(cl)\n",
        "  #\n",
        "  #"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-cijN9B8hfFd"
      },
      "outputs": [],
      "source": [
        "  # 7b) deep learning prediction U-Net ------------------------------------------\n",
        "\n",
        "  ## Config\n",
        "\n",
        "  # config\n",
        "  batch_size = 4\n",
        "  lr_rate = 0.0001\n",
        "\n",
        "  # load data\n",
        "  list_png=list.files(test_dir, pattern = \"*.png$\", full.names = TRUE)\n",
        "\n",
        "  data <- tibble(\n",
        "    img = list_png\n",
        "  )\n",
        "\n",
        "  ## the model\n",
        "\n",
        "  # mixed precision\n",
        "  tf$keras$mixed_precision$experimental$set_policy('mixed_float16')\n",
        "\n",
        "  dice_coef <- custom_metric(\"custom\", function(y_true, y_pred, smooth = 1.0) {\n",
        "    y_true_f <- k_flatten(y_true)\n",
        "    y_pred_f <- k_flatten(y_pred)\n",
        "    intersection <- k_sum(y_true_f * y_pred_f)\n",
        "    result <- (2 * intersection + smooth) /\n",
        "      (k_sum(y_true_f) + k_sum(y_pred_f) + smooth)\n",
        "    return(result)\n",
        "  })\n",
        "\n",
        "  bce_dice_loss <- function(y_true, y_pred) {\n",
        "    result <- loss_binary_crossentropy(y_true, y_pred) +\n",
        "      (1 - dice_coef(y_true, y_pred))\n",
        "    return(result)\n",
        "  }\n",
        "\n",
        "  #get_unet_128 <- function(input_shape = c(640, 640, data_n_layers),\n",
        "  get_unet_128 <- function(input_shape = c(predict_tile_size, predict_tile_size, data_n_layers),\n",
        "                           num_classes = 1) {\n",
        "\n",
        "    inputs <- layer_input(shape = input_shape)\n",
        "    # 128\n",
        "\n",
        "    down1 <- inputs %>%\n",
        "      layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    down1_pool <- down1 %>%\n",
        "      layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "    # 64\n",
        "\n",
        "    down2 <- down1_pool %>%\n",
        "      layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    down2_pool <- down2 %>%\n",
        "      layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "    # 32\n",
        "\n",
        "    down3 <- down2_pool %>%\n",
        "      layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    down3_pool <- down3 %>%\n",
        "      layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "    # 16\n",
        "\n",
        "    down4 <- down3_pool %>%\n",
        "      layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    down4_pool <- down4 %>%\n",
        "      layer_max_pooling_2d(pool_size = c(2, 2), strides = c(2, 2))\n",
        "    # 8\n",
        "\n",
        "    center <- down4_pool %>%\n",
        "      layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 1024, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    # center\n",
        "\n",
        "    up4 <- center %>%\n",
        "      layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "      {layer_concatenate(inputs = list(down4, .), axis = 3)} %>%\n",
        "      layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 512, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    # 16\n",
        "\n",
        "    up3 <- up4 %>%\n",
        "      layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "      {layer_concatenate(inputs = list(down3, .), axis = 3)} %>%\n",
        "      layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 256, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    # 32\n",
        "\n",
        "    up2 <- up3 %>%\n",
        "      layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "      {layer_concatenate(inputs = list(down2, .), axis = 3)} %>%\n",
        "      layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 128, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    # 64\n",
        "\n",
        "    up1 <- up2 %>%\n",
        "      layer_upsampling_2d(size = c(2, 2)) %>%\n",
        "      {layer_concatenate(inputs = list(down1, .), axis = 3)} %>%\n",
        "      layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\") %>%\n",
        "      layer_conv_2d(filters = 64, kernel_size = c(3, 3), padding = \"same\") %>%\n",
        "      layer_batch_normalization() %>%\n",
        "      layer_activation(\"relu\")\n",
        "    # 128\n",
        "\n",
        "    classify <- layer_conv_2d(up1,\n",
        "                              filters = num_classes,\n",
        "                              kernel_size = c(1, 1),\n",
        "                              dtype = 'float32', # mixed precision\n",
        "                              activation = \"sigmoid\")\n",
        "\n",
        "\n",
        "    model <- keras_model(\n",
        "      inputs = inputs,\n",
        "      outputs = classify\n",
        "    )\n",
        "\n",
        "    model %>% compile(\n",
        "      optimizer = optimizer_rmsprop(lr = lr_rate),\n",
        "      loss = bce_dice_loss,\n",
        "      metrics = c(dice_coef)\n",
        "    )\n",
        "\n",
        "    return(model)\n",
        "  }\n",
        "\n",
        "  model <- get_unet_128()\n",
        "\n",
        "  # # function to apply the factor in the imagery\n",
        "  # apply_factor <- function(img) {\n",
        "  #   #img_mask = tf$constant(10000.0, dtype=tf$float32, shape=img$get_shape())\n",
        "  #   #img = img / img_mask\n",
        "  #   img_mask = tf$constant(1e-04, dtype=tf$float32, shape=img$get_shape())\n",
        "  #   img = tf$multiply(img, img_mask)\n",
        "  #   return(img)\n",
        "  # }\n",
        "\n",
        "  # map data\n",
        "  create_dataset <- function(data, batch_size = 4L, data_n_layers = 3) {\n",
        "\n",
        "    dataset <- data %>%\n",
        "      tensor_slices_dataset() %>%\n",
        "      dataset_map(~.x %>% list_modify(\n",
        "        img =  tf$image$decode_png(tf$io$read_file(.x$img), channels=data_n_layers),\n",
        "      )) %>%\n",
        "      # dataset_map(~.x %>% list_modify(\n",
        "      #   img = apply_factor(.x$img)\n",
        "      # ))\n",
        "      dataset_map(~.x %>% list_modify(\n",
        "        img = tf$image$convert_image_dtype(.x$img, dtype = tf$float32),\n",
        "      ))\n",
        "\n",
        "\n",
        "\n",
        "    # train in batches; batch size might need to be adapted depending on\n",
        "    # available memory\n",
        "    dataset <- dataset %>%\n",
        "      dataset_batch(batch_size)\n",
        "\n",
        "    dataset %>%\n",
        "      # output needs to be unnamed\n",
        "      dataset_map(unname)\n",
        "  }\n",
        "\n",
        "  ## Predict\n",
        "\n",
        "  ## load saved weights\n",
        "  load_model_weights_hdf5(model, weights_fname)\n",
        "\n",
        "  # predict\n",
        "  test_dataset <- create_dataset(data, data_n_layers = data_n_layers, batch_size = batch_size)\n",
        "  system.time({ preds <- predict(model, test_dataset) })\n",
        "\n",
        "  print(\"Prediction end.\")\n",
        "\n",
        "  # clear GPU\n",
        "  tf$keras.backend$clear_session()\n",
        "  py_gc <- import('gc')\n",
        "  py_gc$collect()\n",
        "\n",
        "\n",
        "  # load preds\n",
        "  # save(preds, file = \"preds.RData\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P6GRljztho6x"
      },
      "outputs": [],
      "source": [
        "# 7c) create images from DL results ------------------------------------------------\n",
        "  ## ~1min20\n",
        "\n",
        "\n",
        "  # load object from DL prediction\n",
        "  # load(\"preds.RData\", verbose=T)\n",
        "\n",
        "  # set crs\n",
        "\n",
        "\n",
        "  # list files\n",
        "  images_dir = test_dir\n",
        "  images_iter <- list.files(images_dir, pattern = \".png\", full.names = FALSE)#[samples_index]\n",
        "  images_iter2 <- list.files(images_dir, pattern = \".tif\", full.names = TRUE)#[samples_index]\n",
        "\n",
        "  # load one file to get vect_overlap\n",
        "  r_tmp=raster(images_iter2[1])\n",
        "  vect_overlap=c(-(border_size*res(r_tmp)[1]),(border_size*res(r_tmp)[1]),-(border_size*res(r_tmp)[1]),(border_size*res(r_tmp)[1]))\n",
        "\n",
        "  # libraries needed\n",
        "  p_load(parallel, doParallel, foreach)\n",
        "\n",
        "  # Begin cluster\n",
        "  cl = parallel::makeCluster(no_cores) # here you specify the number of processors you want to use, if you dont know you can use detectCores() and ideally use that number minus one\n",
        "  #cl = parallel::makeCluster(3, outfile=\"D:/r_parallel_log.txt\") # if you use this you can see prints in the txt\n",
        "  registerDoParallel(cl)\n",
        "\n",
        "  # get results from the prediction and save it as a raster without overlap (e.g. 512 x 512)\n",
        "  i=1\n",
        "  foreach(i = 1:dim(preds)[1], .inorder=F, .errorhandling='remove') %dopar% {\n",
        "    require(raster)\n",
        "\n",
        "    # classe 1\n",
        "    #new_img1=t(preds[i, , , 1])\n",
        "    new_img1=preds[i, , , 1]\n",
        "    new_img1=ifelse(new_img1 < 0.5, 0, 1)\n",
        "    # classe 2\n",
        "    # new_img2=t(preds[i, , , 2])\n",
        "    # new_img2=ifelse(new_img2 < 0.5, 0, 1)\n",
        "\n",
        "    # put data in an array\n",
        "    #img_array=array(data = NA, dim = c(640,640,1),dimnames = NULL)\n",
        "    img_array=array(data = NA, dim = c(predict_tile_size,predict_tile_size,1),dimnames = NULL)\n",
        "    img_array[,,1]=as.matrix(new_img1)\n",
        "    # img_array[,,2]=as.matrix(new_img1)*0\n",
        "    # img_array[,,3]=as.matrix(new_img2)\n",
        "\n",
        "    ## This was required before we had the tif from gdal_retile\n",
        "    # # get the original geographical file to assign the values\n",
        "    # #r=readAll(raster(images_iter2[i]))\n",
        "    # #r = crop(predictor_data_expand, predictor_data_ext[[i]] + vect_overlap)[[1]]\n",
        "    # r = crop(predictor_data_expand, predictor_data_ext[[i]])[[1]] # no borders\n",
        "    # r[]=img_array[,,1]\n",
        "    #\n",
        "    # # crop to the original extent without overlap\n",
        "    # #r = crop(r, extent(r) - vect_overlap) # no borders\n",
        "    # r = crop(r, extent(r))\n",
        "\n",
        "    ## Now we use the tif fiels\n",
        "    # load the tif with geospatial information\n",
        "    r = raster(images_iter2[i])\n",
        "    r[] = img_array[,,1]\n",
        "\n",
        "    # crop the borders\n",
        "    r=crop(r, extent(r)-vect_overlap)\n",
        "    crs(r) = crs(data_crs)\n",
        "\n",
        "    writeRaster(r, paste0(prediction_data_dir, \"./pred_output/\", sub_extension(images_iter[i], \".tif\")), datatype='INT1U', overwrite=TRUE)\n",
        "    #print(i)\n",
        "\n",
        "  }\n",
        "\n",
        "  # finish cluster\n",
        "  stopCluster(cl)\n",
        "\n",
        "  # msg\n",
        "  print(\"Prediction images saved.\")\n",
        "\n",
        "  # clear prediction file\n",
        "  rm(preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZiCKTxrhxOt"
      },
      "outputs": [],
      "source": [
        "  # 7d) mosaic images -----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "  # list files\n",
        "  list_datamask=list.files(paste0(getwd(), \"/\", prediction_data_dir, \"/pred_output/\"), full.names = TRUE, pattern=\".tif$\")\n",
        "  sink(paste0(prediction_data_dir,\"\\\\tmp\\\\file_list.txt\"))\n",
        "  cat(list_datamask, sep=\"\\n\")\n",
        "  sink()\n",
        "  #list_datamask=unlist(list.files(paste0(prediction_data_dir, \"./pred_output/\"), full.names = TRUE, pattern=\".tif$\"))\n",
        "  #list_datamask=paste(list_datamask,collapse =\" \")\n",
        "  name_final_vrt=paste(prediction_data_dir, \"./pred_mosaic/\", result_fname, \"_\",sub_extension(basename(img_list[i_img]),\"\"),\".vrt\",sep=\"\")\n",
        "  name_final_TIF=paste(prediction_data_dir, \"./pred_mosaic/\", result_fname, \"_\",sub_extension(basename(img_list[i_img]),\"\"),\".tif\",sep=\"\")\n",
        "\n",
        "  # mosaic\n",
        "  system(paste(gdalbuildvrt, \"-input_file_list\", paste0(prediction_data_dir,\"\\\\tmp\\\\file_list.txt\"), name_final_vrt ))\n",
        "  #system(paste(gdalbuildvrt, name_final_vrt ,list_datamask ))\n",
        "  system(paste(gdal_translate, \"-co COMPRESS=NONE -a_nodata 255\", name_final_vrt, name_final_TIF))\n",
        "\n",
        "  # plot(raster(name_final_TIF))\n",
        "\n",
        "\n",
        "  #\n",
        "  gc()\n",
        "\n",
        "  #\n",
        "  print(\"Mosaic done.\")\n",
        "\n",
        "\n",
        "  # adjust image to fit the JPG\n",
        "  r = raster(name_final_TIF)\n",
        "  shift(r, dy = -3000, filename = sub_extension(name_final_TIF,\"_adjusted.tif\"), overwrite=T)\n",
        "  #shift(r, dy = -3000, filename = name_final_TIF, overwrite=T)\n",
        "  rm(r)\n",
        "  unlink(name_final_TIF)\n",
        "  file.rename(sub_extension(name_final_TIF,\"_adjusted.tif\"), name_final_TIF)\n",
        "\n",
        "\n",
        "  # clear intermediate files\n",
        "  unlink(list.files(paste0(prediction_data_dir, \"./tmp/\"), full.names = TRUE))\n",
        "  unlink(list.files(paste0(prediction_data_dir, \"./pred_input/\"), full.names = TRUE))\n",
        "  unlink(list.files(paste0(prediction_data_dir, \"./pred_output/\"), full.names = TRUE))\n",
        "  #unlink(list.files(paste0(prediction_data_dir, \"./pred_mosaic/\"), full.names = TRUE)) # optional if you want to keep the mosaic\n",
        "  gc()\n",
        "  print(Sys.time())\n",
        "  print(\"Finished.\")\n",
        "\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYuqNI-jh5Z2"
      },
      "outputs": [],
      "source": [
        "# 8) compare results visually ---------------------------------------------------------\n",
        "\n",
        "\n",
        "# load U-Net single\n",
        "unet_map = raster(\"6_sampling1_prediction_singledate\\\\pred_mosaic\\\\rice_map_single.tif\")\n",
        "names(unet_map) = \"rice_map\"\n",
        "\n",
        "# load RF\n",
        "rf_map = crop(raster(\"s2_stack_classified_rf_2019_2020_singledate.tif\"), unet_map)\n",
        "\n",
        "\n",
        "# visualize\n",
        "plot(rf_map, main = \"rf\")\n",
        "plot(unet_map, main = \"unet\")\n",
        "\n",
        "\n",
        "# bigger screen of rf and unet\n",
        "x11()\n",
        "plot(rf_map, main = \"rf\")\n",
        "x11()\n",
        "plot(unet_map, main = \"unet\")\n",
        "\n",
        "\n",
        "\n",
        "# visualize in a zoomed area\n",
        "# new_ext = drawExtent()\n",
        "# dput(new_ext)\n",
        "#\n",
        "# area 1\n",
        "new_ext = new(\"Extent\", xmin = 514949.863470392, xmax = 546954.943641848,\n",
        "              ymin = 6726688.02349579, ymax = 6741196.99317352)\n",
        "#plot(rf_map)\n",
        "#lines(rasext_to_sp(new_ext))\n",
        "\n",
        "x11()\n",
        "plot(crop(rf_map,new_ext), main = \"rf\")\n",
        "lines(crop(field_data, new_ext))\n",
        "x11()\n",
        "plot(crop(unet_map,new_ext), main = \"unet\")\n",
        "lines(crop(field_data, new_ext))\n",
        "\n",
        "\n",
        "# area 2\n",
        "new_ext = new(\"Extent\", xmin = 475690.298460073, xmax = 502574.565804096,\n",
        "              ymin = 6675479.89522146, ymax = 6694256.20892205)\n",
        "#plot(rf_map)\n",
        "#lines(rasext_to_sp(new_ext))\n",
        "\n",
        "x11()\n",
        "plot(crop(rf_map,new_ext), main = \"rf\")\n",
        "lines(crop(field_data, new_ext))\n",
        "x11()\n",
        "plot(crop(unet_map,new_ext), main = \"unet\")\n",
        "lines(crop(field_data, new_ext))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sz7oz_cRh8ge"
      },
      "outputs": [],
      "source": [
        "# 9) compare results quantitatively ----------------------------------------------\n",
        "\n",
        "# load U-Net single\n",
        "unet_map = raster(\"6_sampling1_prediction_singledate\\\\pred_mosaic\\\\rice_map_single.tif\")\n",
        "names(unet_map) = \"rice_map\"\n",
        "\n",
        "# load RF\n",
        "rf_map = crop(raster(\"s2_stack_classified_rf_2019_2020_singledate.tif\"), unet_map)\n",
        "\n",
        "\n",
        "# load validation data\n",
        "load(\"data_df_split_singledate.RData\", verbose=T)\n",
        "length(data_df_valid$class)\n",
        "\n",
        "# load ML models\n",
        "load(\"model_rf_singledate.RData\", verbose=T)\n",
        "fit_rf_single = fit\n",
        "\n",
        "# predict results for validation data\n",
        "fit_rf_single_pred = predict(fit_rf_single, data_df_valid)\n",
        "length(fit_rf_single_pred)\n",
        "\n",
        "\n",
        "## do the same for the deep learning\n",
        "# get the polygons that were not used in training\n",
        "load(\"deep_learning_patch_samples_singledate.RData\", verbose=T)\n",
        "#\n",
        "samples_patches_validation = crop(samples_patches_validation, unet_map)\n",
        "\n",
        "# sample inside the tiles\n",
        "set.seed(1)\n",
        "i=1\n",
        "for (i in 1:length(samples_patches_validation)) {\n",
        "  tmp = crop(unet_map, samples_patches_validation[i,])\n",
        "  tmp2 = sampleRandom(tmp, 4, sp=T)\n",
        "  tmp3 = aggregate(crop(field_data, samples_patches_validation[i,]))\n",
        "  # plot(tmp)\n",
        "  # points(tmp2, col=\"red\")\n",
        "  # lines(tmp3)\n",
        "  #\n",
        "  ref = as.numeric(gIntersects(tmp2, tmp3, byid=T))\n",
        "  map = tmp2$rice_map\n",
        "\n",
        "  if (i == 1) {\n",
        "    validation_points = cbind(ref,map)\n",
        "  } else {\n",
        "    validation_points = rbind(validation_points, cbind(ref,map))\n",
        "  }\n",
        "  print(i)\n",
        "}\n",
        "validation_points = data.frame(validation_points)\n",
        "validation_points$ref = as.factor(validation_points$ref)\n",
        "validation_points$map = as.factor(validation_points$map)\n",
        "\n",
        "# verify results with confusion matrices\n",
        "confusionMatrix(data = fit_rf_single_pred, reference = data_df_valid$class, mode=\"prec_recall\", positive = \"Rice\")\n",
        "confusionMatrix(data = validation_points$map, reference = validation_points$ref, mode=\"prec_recall\", positive = \"1\")\n",
        "\n",
        "# visualize some validation patches\n",
        "i=12\n",
        "x11()\n",
        "par(mfrow = c(4,4), mar = c(1,1,1,1))\n",
        "for (i in 1:16+50) {\n",
        "  plot(crop(unet_map, samples_patches_validation[i,]))\n",
        "  lines(crop(field_data, samples_patches_validation[i,]))\n",
        "  legend(\"topright\", legend = \"unet\", bty=\"n\")\n",
        "}\n",
        "\n",
        "\n",
        "# visualize some validation patches - RF\n",
        "x11()\n",
        "i=12\n",
        "par(mfrow = c(4,4), mar = c(1,1,1,1))\n",
        "for (i in 1:16+50) {\n",
        "  plot(crop(rf_map, samples_patches_validation[i,]))\n",
        "  lines(crop(field_data, samples_patches_validation[i,]))\n",
        "  legend(\"topright\", legend = \"rf\", bty=\"n\")\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kz5ME35iEUT"
      },
      "outputs": [],
      "source": [
        "# 10) Final Considerations ---------------------------------------------------\n",
        "\n",
        "## things to consider when interpreting these results:\n",
        "## 1) Samples for RF considered for target-class -30 m buffer and bigger polygons -> overestimated performance\n",
        "## 2) The RF showed lots of noise across the image which were not accounted in the validation -> use better ways to validate based on polygons\n",
        "## 3) No tuning was done for the DL method -> can improve\n",
        "## 4) We did not refine the samples for DL, and this affect the DL learning -> can improve\n",
        "## 5) This is an easy target to detect."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "R",
      "name": "ir"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}